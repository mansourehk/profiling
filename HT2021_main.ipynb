{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HT2021-main.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lwg8OXc8l6qs",
        "outputId": "8bbe1376-ebb1-4aeb-ac51-7cc447c96b49"
      },
      "source": [
        "from datetime import datetime\n",
        "print(\"The last time this cell was running: {}\".format(datetime.now().strftime('%Y-%m-%d %H:%M:%S')))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The last time this cell was running: 2021-05-05 12:02:13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7iQi4Hemrf_",
        "outputId": "e029c426-0de6-4e74-9573-dafc15650f03"
      },
      "source": [
        "!pip install twokenize\n",
        "!pip install ijson\n",
        "!pip install statistics"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: twokenize in /home/local/ASUAD/mkarami/anaconda3/lib/python3.8/site-packages (1.0.0)\n",
            "Requirement already satisfied: ijson in /home/local/ASUAD/mkarami/anaconda3/lib/python3.8/site-packages (3.1.3)\n",
            "Requirement already satisfied: statistics in /home/local/ASUAD/mkarami/anaconda3/lib/python3.8/site-packages (1.0.3.5)\n",
            "Requirement already satisfied: docutils>=0.3 in /home/local/ASUAD/mkarami/anaconda3/lib/python3.8/site-packages (from statistics) (0.16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRMlNVzRlikU"
      },
      "source": [
        "Needed Libraries and Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUqsTtFnl6qs"
      },
      "source": [
        "import os\n",
        "import json\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import numpy as np\n",
        "import ijson\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "from statistics import mean\n",
        "from datetime import datetime\n",
        "import ast\n",
        "\n",
        "anx_cat=128\n",
        "discrep_cat=134\n",
        "tentat_cat=135\n",
        "certain_cat=136\n",
        "swear_cat=22\n",
        "future_cat=15\n",
        "politifact_fake_users = {}\n",
        "politifact_real_users = {}\n",
        "gossipcop_fake_users = {}\n",
        "gossipcop_real_users = {}\n",
        "retweet_counts = {} #tweetID:retweet_count #for fake/real news tweets not user timelines\n",
        "like_counts = {} #tweetID:favorite_count #for fake/real news tweets not user timelines"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7B8U1tTlfk2"
      },
      "source": [
        "Extract the base directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "912gHxGK0xIu",
        "outputId": "9e6ebef6-0608-430e-c577-44908e00cb26"
      },
      "source": [
        "# # In case running from google colab\n",
        "# try:\n",
        "#     from google.colab import drive\n",
        "#     drive.mount('/content/drive')\n",
        "#     base_dir = 'drive/Shared drives/Profiling Fake News Spreaders'\n",
        "# except:\n",
        "#     base_dir = './'\n",
        "base_dir = os.getcwd() + '/'\n",
        "print('Base Dir:', base_dir)  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Base Dir: /home/local/ASUAD/mkarami/Documents/Motivational Factors/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzOodz-3mN5o"
      },
      "source": [
        "Read and counts the retweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oA8mcyMZl6qs"
      },
      "source": [
        "def read_retweets(dir):\n",
        "    articles = os.listdir(dir)\n",
        "    for article in articles:\n",
        "        if os.path.isdir(dir + '/' + article):\n",
        "            fin = open(dir + '/' + article + '/retweets.json')\n",
        "            for line in fin:\n",
        "                tweets = json.loads(line)\n",
        "                for tweet_id in tweets:\n",
        "                    retweet_counts[tweet_id] = len(tweets[tweet_id])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rcUsqx-FaPl",
        "outputId": "24213685-e87a-4d38-e2e5-2d16971e6acc"
      },
      "source": [
        "dir = base_dir + 'gossipcop_real'\n",
        "folders = os.listdir(dir)\n",
        "print(len(folders))\n",
        "for folder in folders:\n",
        "    if os.path.isdir(dir + '/' + folder):\n",
        "        files = os.listdir(dir + '/' + folder)\n",
        "    if len(files)<5:\n",
        "        print(folder)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16817\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQ0VwLi4H79S"
      },
      "source": [
        "dir = base_dir + 'gossipcop_fake'\n",
        "folders = os.listdir(dir)\n",
        "for folder in folders:\n",
        "    if os.path.isdir(dir + '/' + folder):\n",
        "        files = os.listdir(dir + '/' + folder)\n",
        "    if len(files)<5:\n",
        "        print(folder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSVZYXHv6gNx"
      },
      "source": [
        "read_retweets(base_dir + 'politifact_real')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZEpOGsUlfk4"
      },
      "source": [
        "read_retweets(base_dir + 'politifact_fake')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiAY1AgFszhN"
      },
      "source": [
        "read_retweets(base_dir + 'gossipcop_real')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmjToHUg6AiG"
      },
      "source": [
        "read_retweets(base_dir + 'gossipcop_fake')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2rVLIjomifd"
      },
      "source": [
        "Read the articles and counts the likes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvWReUtE6DIM"
      },
      "source": [
        "def read_likes(dir):\n",
        "    articles = os.listdir(dir)\n",
        "    for article in articles:\n",
        "        if os.path.isdir(dir + '/' + article):\n",
        "            fin = open(dir + '/' + article + '/likes.json')\n",
        "            for line in fin:\n",
        "                tweets = json.loads(line)\n",
        "                for tweet_id in tweets:\n",
        "                    like_counts[tweet_id] = len(tweets[tweet_id])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23hpimkvlfk5"
      },
      "source": [
        "read_likes(base_dir + 'politifact_real')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnrMpoRplfk5"
      },
      "source": [
        "read_likes(base_dir + 'gossipcop_real')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8LhhH1Klfk5"
      },
      "source": [
        "read_likes(base_dir + 'politifact_fake')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWmWxKvalfk5"
      },
      "source": [
        "read_likes(base_dir + 'gossipcop_fake')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_hq_odjm-xb"
      },
      "source": [
        "For each user, find all his/her tweets and filter those with more than 3 tweets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "2GSGB4_Rl6qt",
        "outputId": "78bc11e9-a4ab-4f26-a5bd-29d98d81e5c9"
      },
      "source": [
        "def read_user_tweets(dir, users, out_file, activity_cutoff=1): # cut off users with less than activity_cutoff fake/real tweets\n",
        "    articles = os.listdir(dir)\n",
        "    total_tweets = 0\n",
        "    local_retweet_counts = {} #user:[retweet_count_of_fake_tweets]\n",
        "    local_likes_count = {} #user:[favorited_count_of_fake_tweets]\n",
        "    for article in articles:\n",
        "        if os.path.isdir(dir + '/' + article):\n",
        "            fin = open(dir + '/' + article + '/tweets.json')\n",
        "            tweets = json.loads(fin.readline(), strict=False)['tweets']\n",
        "            for tweet in tweets:\n",
        "                user_id = str(tweet['user_id'])\n",
        "                total_tweets += 1\n",
        "                users[user_id] = users.get(user_id,{'text':[]})\n",
        "                users[user_id]['text'].append(tweet['text'])\n",
        "                if user_id not in local_retweet_counts:\n",
        "                    local_retweet_counts[user_id] = [retweet_counts[str(tweet['tweet_id'])]]\n",
        "                else:\n",
        "                    temp = local_retweet_counts[user_id]\n",
        "                    temp.append(retweet_counts[str(tweet['tweet_id'])])\n",
        "                    local_retweet_counts[user_id] = temp[:]\n",
        "                \n",
        "                if user_id not in local_likes_count:\n",
        "                    local_likes_count[user_id] = [like_counts[str(tweet['tweet_id'])]]\n",
        "                else:\n",
        "                    temp = local_likes_count[user_id]\n",
        "                    temp.append(like_counts[str(tweet['tweet_id'])])\n",
        "                    local_likes_count[user_id] = temp[:]\n",
        "    for user in users:\n",
        "        users[user]['avg_fake_retweet_count'] = mean(local_retweet_counts[user])\n",
        "        users[user]['avg_fake_favorite_count'] = mean(local_likes_count[user])\n",
        "                \n",
        "    users_filtered = {}\n",
        "    for user in users:\n",
        "        if len(users[user]['text']) >= activity_cutoff:\n",
        "            users_filtered[user] = users[user]\n",
        "    pickle.dump(users_filtered, open(out_file, 'wb'))\n",
        "    print('Number of users with {} or more tweets is {}\\n'.format(activity_cutoff, len(users_filtered)))\n",
        "    return users_filtered\n",
        "\n",
        "politifact_fake_users = {}\n",
        "politifact_real_users = {}\n",
        "gossipcop_fake_users = {}\n",
        "gossipcop_real_users = {}\n",
        "politifact_real_users = read_user_tweets(base_dir + 'politifact_real', politifact_real_users, base_dir + 'Filtered_date_users_with_three_tweets_or_more/politifact_real_users.pkl', 3)\n",
        "gossipcop_real_users = read_user_tweets(base_dir +'gossipcop_real', gossipcop_real_users, base_dir +'/Filtered_date_users_with_three_tweets_or_more/gossipcop_real_users.pkl', 3)\n",
        "politifact_fake_users = read_user_tweets(base_dir + 'politifact_fake', politifact_fake_users, base_dir + 'Filtered_date_users_with_three_tweets_or_more/politifact_fake_users.pkl', 3)\n",
        "gossipcop_fake_users = read_user_tweets(base_dir +'gossipcop_fake', gossipcop_fake_users, base_dir +'/Filtered_date_users_with_three_tweets_or_more/gossipcop_fake_users.pkl', 3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of users with 3 or more tweets is 12571\n",
            "\n",
            "Number of users with 3 or more tweets is 2831\n",
            "\n",
            "Number of users with 3 or more tweets is 6322\n",
            "\n",
            "Number of users with 3 or more tweets is 29384\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aorHiAcfnSnx"
      },
      "source": [
        "Save the info extracted from the profiles of the users"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTpD4y9gl6qt",
        "outputId": "b4e3319c-9ba0-4a23-d96c-7046cb616c2b"
      },
      "source": [
        "def read_users_profiles(in_file, users, out_file):\n",
        "    profiles_dict = {}\n",
        "    fin = open(in_file)\n",
        "    profiles = ijson.items(fin, 'user_profiles.item')\n",
        "    for profile in tqdm(profiles):\n",
        "        profiles_dict[profile['id_str']] = {'friends_count':profile['friends_count'], 'followers_count':profile['followers_count']}\n",
        "    for user in tqdm(users):\n",
        "        profile = profiles_dict.get(user, {'friends_count':None, 'followers_count':None})\n",
        "        users[user]['friends_count'] = profile['friends_count']\n",
        "        users[user]['followers_count'] = profile['followers_count']\n",
        "    \n",
        "    pickle.dump(profiles_dict, open(out_file, 'wb'))\n",
        "\n",
        "read_users_profiles(base_dir + '/User profile data/gossipcop_real_complete_user_profile_meta_data.json', \n",
        "                    gossipcop_real_users, \n",
        "                    base_dir + 'Filtered_date_users_with_three_tweets_or_more/gossipcop_real_users_profile.pkl')\n",
        "\n",
        "read_users_profiles(base_dir +'/User profile data/gossipcop_fake_complete_user_profile_meta_data.json', \n",
        "                    gossipcop_fake_users, \n",
        "                    base_dir +'/Filtered_date_users_with_three_tweets_or_more/gossipcop_fake_users_profile.pkl')\n",
        "\n",
        "read_users_profiles(base_dir + 'User profile data/politifact_real_complete_user_profile_meta_data.json', \n",
        "                    politifact_real_users, \n",
        "                    base_dir + 'Filtered_date_users_with_three_tweets_or_more/politifact_real_users_profile.pkl')\n",
        "\n",
        "read_users_profiles(base_dir + 'User profile data/politifact_fake_complete_user_profile_meta_data.json', \n",
        "                    politifact_fake_users, \n",
        "                    base_dir + 'Filtered_date_users_with_three_tweets_or_more/politifact_fake_users_profile.pkl')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "69069it [00:02, 24843.68it/s]\n",
            "100%|██████████| 2831/2831 [00:00<00:00, 596537.28it/s]\n",
            "518004it [00:21, 23877.81it/s]\n",
            "100%|██████████| 29384/29384 [00:00<00:00, 828457.16it/s]\n",
            "688956it [00:26, 26012.57it/s]\n",
            "100%|██████████| 12571/12571 [00:00<00:00, 648870.84it/s]\n",
            "212736it [00:08, 25940.97it/s]\n",
            "100%|██████████| 6322/6322 [00:00<00:00, 675404.73it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6U2AqLcniez"
      },
      "source": [
        "Extract the users' timeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0HQpEfwSl6q4",
        "outputId": "70dd4bd7-6caf-4c78-9450-6e5c1b9393b0"
      },
      "source": [
        "def read_users_timeline(in_file_timelines, out_file_tweet, out_file_retweet, out_file_favorite, out_file_avg_twt):\n",
        "    timelines = {} # user:[tweets]\n",
        "    retweet_counts = {} # user:avg_retweet_count\n",
        "    favorite_counts = {} # user:avg_favorited_count\n",
        "    avg_twt_count = {} # user:avg_twt_per_day\n",
        "    for line in tqdm(open(in_file_timelines, encoding='windows-1252')):\n",
        "        try:\n",
        "            line = json.loads(line)\n",
        "        except:\n",
        "            continue\n",
        "        if line is not None:\n",
        "            avg_retweet_count = []\n",
        "            avg_favorite_count = []\n",
        "            twt_time = []\n",
        "            timelines[str(line['user_id'])] = []\n",
        "            if line['recent_tweets'] is not None:\n",
        "                for tweet in line['recent_tweets']:\n",
        "                    avg_retweet_count.append(int(tweet['retweet_count']))\n",
        "                    avg_favorite_count.append(int(tweet['favorite_count']))\n",
        "                    twt_time.append(datetime.strptime(tweet['created_at'], \"%a %b %d %H:%M:%S +0000 %Y\"))\n",
        "                    timelines[str(line['user_id'])].append(tweet['text'])\n",
        "            if len(avg_retweet_count) == 0:\n",
        "                retweet_counts[str(line['user_id'])] = None\n",
        "            else:\n",
        "                retweet_counts[str(line['user_id'])] = mean(avg_retweet_count)\n",
        "            if len(avg_favorite_count) == 0:\n",
        "                favorite_counts[str(line['user_id'])] = None\n",
        "            else:\n",
        "                favorite_counts[str(line['user_id'])] = mean(avg_favorite_count)\n",
        "            twt_time = sorted(twt_time)\n",
        "            if len(twt_time) == 0:\n",
        "                avg_twt_count[str(line['user_id'])] = None\n",
        "            else:\n",
        "                avg_twt_count[str(line['user_id'])] = len(twt_time)/float((twt_time[-1] - twt_time[0]).days+1)\n",
        "    pickle.dump(timelines, open(out_file_tweet, 'wb'))\n",
        "    pickle.dump(retweet_counts, open(out_file_retweet, 'wb'))\n",
        "    pickle.dump(favorite_counts, open(out_file_favorite, 'wb'))\n",
        "    pickle.dump(avg_twt_count, open(out_file_avg_twt, 'wb'))\n",
        "\n",
        "read_users_timeline(base_dir +'fakenewsnet_user_recent_tweets', \n",
        "                    base_dir +'fakenewsnet_user_recent_tweets_text_only.pkl',\n",
        "                    base_dir +'fakenewsnet_user_retweet_count.pkl',\n",
        "                    base_dir +'fakenewsnet_user_favorite_count.pkl',\n",
        "                    base_dir +'fakenewsnet_user_avg_twt_count_test.pkl')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50811it [09:30, 89.12it/s] \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j91wCOFul6q6"
      },
      "source": [
        "import codecs\n",
        "tweets = pickle.load(open(base_dir +'fakenewsnet_user_recent_tweets_text_only.pkl', 'rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "GHwB3Hcul6q6",
        "outputId": "be9a3745-ec78-4f27-98b7-398a5abf84b9"
      },
      "source": [
        "import csv\n",
        "for user in tqdm(tweets):\n",
        "    outfile = codecs.open(base_dir +'Tweets_for_LIWC/{}.txt'.format(user), 'w', encoding='utf-8')\n",
        "    temp_tweets = ' '.join(tweets[user])\n",
        "    temp_tweets = temp_tweets.replace('\\n', ' ')\n",
        "    outfile.write(temp_tweets)\n",
        "    outfile.write('\\n')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 29038/29038 [00:02<00:00, 10863.97it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_3eRpQTl6q6",
        "outputId": "010ab2b2-13ec-4242-b701-30f60a8b87c5"
      },
      "source": [
        "tweets = pickle.load(open(base_dir +'/fakenewsnet_user_recent_tweets_text_only.pkl', 'rb'))\n",
        "retweet_counts = pickle.load(open(base_dir +'/fakenewsnet_user_retweet_count.pkl', 'rb'))\n",
        "favorite_counts = pickle.load(open(base_dir +'/fakenewsnet_user_favorite_count.pkl', 'rb'))\n",
        "avg_twt_count = pickle.load(open(base_dir +'/fakenewsnet_user_avg_twt_count.pkl', 'rb'))\n",
        "null_users = 0\n",
        "\n",
        "for user in tqdm(gossipcop_real_users):\n",
        "    if user in tweets:\n",
        "        gossipcop_real_users[user]['timeline'] = tweets[user]\n",
        "        gossipcop_real_users[user]['avg_retweet_count'] = retweet_counts[user]\n",
        "        gossipcop_real_users[user]['avg_favorite_count'] = favorite_counts[user]\n",
        "        gossipcop_real_users[user]['avg_twt_count'] = avg_twt_count[user]\n",
        "    else:\n",
        "        null_users += 1\n",
        "        gossipcop_real_users[user]['timeline'] = None\n",
        "        gossipcop_real_users[user]['avg_retweet_count'] = None\n",
        "        gossipcop_real_users[user]['avg_favorite_count'] = None\n",
        "        gossipcop_real_users[user]['avg_twt_count'] = None\n",
        "print ('Number of null users in gossipcop_real_users is {}'.format(null_users))\n",
        "null_users = 0\n",
        "\n",
        "for user in tqdm(gossipcop_fake_users):\n",
        "    if user in tweets:\n",
        "        gossipcop_fake_users[user]['timeline'] = tweets[user]\n",
        "        gossipcop_fake_users[user]['avg_retweet_count'] = retweet_counts[user]\n",
        "        gossipcop_fake_users[user]['avg_favorite_count'] = favorite_counts[user]\n",
        "        gossipcop_fake_users[user]['avg_twt_count'] = avg_twt_count[user]\n",
        "    else:\n",
        "        null_users += 1\n",
        "        gossipcop_fake_users[user]['timeline'] = None\n",
        "        gossipcop_fake_users[user]['avg_retweet_count'] = None\n",
        "        gossipcop_fake_users[user]['avg_favorite_count'] = None\n",
        "        gossipcop_fake_users[user]['avg_twt_count'] = None\n",
        "\n",
        "print ('Number of null users in gossipcop_fake_users is {}'.format(null_users))\n",
        "null_users = 0\n",
        "\n",
        "for user in tqdm(politifact_real_users):\n",
        "    if user in tweets:\n",
        "        politifact_real_users[user]['timeline'] = tweets[user]\n",
        "        politifact_real_users[user]['avg_retweet_count'] = retweet_counts[user]\n",
        "        politifact_real_users[user]['avg_favorite_count'] = favorite_counts[user]\n",
        "        politifact_real_users[user]['avg_twt_count'] = avg_twt_count[user]\n",
        "    else:\n",
        "        null_users += 1\n",
        "        politifact_real_users[user]['timeline'] = None\n",
        "        politifact_real_users[user]['avg_retweet_count'] = None\n",
        "        politifact_real_users[user]['avg_favorite_count'] = None\n",
        "        politifact_real_users[user]['avg_twt_count'] = None\n",
        "print ('Number of null users in politifact_real_users is {}'.format(null_users))\n",
        "null_users = 0\n",
        "        \n",
        "        \n",
        "for user in tqdm(politifact_fake_users):\n",
        "    if user in tweets:\n",
        "        politifact_fake_users[user]['timeline'] = tweets[user]\n",
        "        politifact_fake_users[user]['avg_retweet_count'] = retweet_counts[user]\n",
        "        politifact_fake_users[user]['avg_favorite_count'] = favorite_counts[user]\n",
        "        politifact_fake_users[user]['avg_twt_count'] = avg_twt_count[user]\n",
        "    else:\n",
        "        null_users += 1\n",
        "        politifact_fake_users[user]['timeline'] = None\n",
        "        politifact_fake_users[user]['avg_retweet_count'] = None\n",
        "        politifact_fake_users[user]['avg_favorite_count'] = None\n",
        "        politifact_fake_users[user]['avg_twt_count'] = None\n",
        "print('Number of null users in politifact_fake_users is {}'.format(null_users))\n",
        "null_users = 0\n",
        "        "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2831/2831 [00:00<00:00, 445840.67it/s]\n",
            "100%|██████████| 29384/29384 [00:00<00:00, 636289.35it/s]\n",
            "100%|██████████| 12571/12571 [00:00<00:00, 432032.94it/s]\n",
            "100%|██████████| 6322/6322 [00:00<00:00, 308190.35it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of null users in gossipcop_real_users is 37\n",
            "Number of null users in gossipcop_fake_users is 17824\n",
            "Number of null users in politifact_real_users is 60\n",
            "Number of null users in politifact_fake_users is 13\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fX8th2EoBeb"
      },
      "source": [
        "Preparing LIWC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QnPsSX6l6q7"
      },
      "source": [
        "# LIWC\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "import string\n",
        "def tokenize(text):\n",
        "\tlda_tokens = []\n",
        "\ttknzr = TweetTokenizer()\n",
        "\ttokens = tknzr.tokenize(text)\n",
        "\tfor token in tokens:\n",
        "\t\ttoken.replace(string.whitespace, '')\n",
        "\t\tif len(token) == 0:\n",
        "\t\t\tcontinue\n",
        "\t\telif not token.startswith('http') and not token.startswith('@'):\n",
        "\t\t\tlda_tokens.append(token.lower())\n",
        "\tlda_tokens = [token for token in lda_tokens if len(token) > 4]\n",
        "\treturn lda_tokens\n",
        "\n",
        "\n",
        "def load_liwc(liwc_words_file, category):\n",
        "    words = []\n",
        "    for line in open(liwc_words_file, 'r'):\n",
        "        line = line.strip().split('\\t')\n",
        "        for cat in line[1:]:\n",
        "            if int(cat) == category:\n",
        "                words.append(line[0])\n",
        "                break\n",
        "    return words\n",
        "\n",
        "def liwc_analysis(text, cat_words):\n",
        "\n",
        "    if len(text) == 0 or (len(text) == 1 and text[0] == ''):\n",
        "        return None\n",
        "\n",
        "    count = 0\n",
        "    for word in text:\n",
        "        if word in cat_words:\n",
        "            count += 1\n",
        "            continue\n",
        "        for cat_word in cat_words:\n",
        "            if '*' not in cat_word:\n",
        "                continue\n",
        "            cat_word = cat_word[:-1]\n",
        "            if word.startswith(cat_word) and len(word)-len(cat_word) < 5:\n",
        "                count += 1\n",
        "                break\n",
        "    return float(count)/len(text)\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower().replace(\"'t\",'').replace(\"'ll\",'').replace(\"'ve\", '').replace(\"'m\",'').replace(\"'d\",'').replace(\"'s\",'').replace(\"'re\",'')\n",
        "    cleaned_text = []\n",
        "    lmtzr = WordNetLemmatizer()\n",
        "    for t in tokenize(text):\n",
        "        remove_punc = dict((ord(char), None) for char in '!\"$%&()*+,-./:;<=>?[]^_`{|}~')\n",
        "        t = t.translate(remove_punc)\n",
        "        if r'@' not in t and r'#' not in t and r'http' not in t and r'https' not in t and r'\\u' not in t and t != 'rt' and not t.isdigit() and len(t) > 2:\n",
        "            t = lmtzr.lemmatize(t)\n",
        "            cleaned_text.append(t)\n",
        "    return cleaned_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-3WijBFBk_q",
        "outputId": "143f5e95-2dc2-4033-9d47-0cac0fead6cf"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /home/local/ASUAD/mkarami/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EF532fOooKmM"
      },
      "source": [
        "Calculate the motivational features (Figure one in the paper)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VqHi-VHLl6q7",
        "outputId": "cfee435e-62b1-4d16-f9a4-8b39e37c0451"
      },
      "source": [
        "def calc_features(users, out_file):\n",
        "    features = {}\n",
        "    anx_words = load_liwc(base_dir +'LIWC2007_Words.txt', anx_cat)\n",
        "    discrep_words = load_liwc(base_dir +'LIWC2007_Words.txt', discrep_cat)\n",
        "    tentat_words = load_liwc(base_dir +'LIWC2007_Words.txt', tentat_cat)\n",
        "    certain_words = load_liwc(base_dir +'LIWC2007_Words.txt', certain_cat)\n",
        "    swear_words = load_liwc(base_dir +'LIWC2007_Words.txt', swear_cat)\n",
        "    future_words = load_liwc(base_dir +'LIWC2007_Words.txt', future_cat)\n",
        "    \n",
        "    for user in tqdm(users):\n",
        "        if users[user]['timeline'] is not None:\n",
        "            text = clean_text(' '.join(users[user]['timeline']))\n",
        "            features[user] = {}\n",
        "            features[user]['anx'] = liwc_analysis(text, anx_words)\n",
        "            features[user]['discrep'] = liwc_analysis(text, discrep_words)\n",
        "            features[user]['tentat'] = liwc_analysis(text, tentat_words)\n",
        "            features[user]['certain'] = liwc_analysis(text, certain_words)\n",
        "            features[user]['swear'] = liwc_analysis(text, swear_words)\n",
        "            features[user]['future'] = liwc_analysis(text, future_words)\n",
        "            features[user]['friends'] = users[user]['friends_count']\n",
        "            features[user]['followers'] = users[user]['followers_count']\n",
        "            features[user]['avg_twt'] = users[user]['avg_twt_count']\n",
        "            if users[user]['avg_retweet_count'] is not None:\n",
        "                features[user]['rel_enhance_retweet'] = users[user]['avg_retweet_count'] - users[user]['avg_fake_retweet_count']\n",
        "            else:\n",
        "                features[user]['rel_enhance_retweet'] = None\n",
        "            if users[user]['avg_favorite_count'] is not None:\n",
        "                features[user]['rel_enhance_favorite'] = users[user]['avg_favorite_count'] - users[user]['avg_fake_favorite_count']\n",
        "            else: \n",
        "                features[user]['rel_enhance_favorite'] = None\n",
        "        else:\n",
        "            features[user] = {}\n",
        "            features[user]['anx'] = None\n",
        "            features[user]['discrep'] = None\n",
        "            features[user]['tentat'] = None\n",
        "            features[user]['certain'] = None\n",
        "            features[user]['swear'] = None\n",
        "            features[user]['future'] = None\n",
        "            features[user]['friends'] = users[user]['friends_count']\n",
        "            features[user]['followers'] = users[user]['followers_count']\n",
        "            features[user]['avg_twt'] = None\n",
        "            features[user]['rel_enhance_retweet'] = None\n",
        "            features[user]['rel_enhance_favorite'] = None\n",
        "#     print(features)        \n",
        "    pickle.dump(features, open(out_file, 'wb'))\n",
        "        \n",
        "calc_features(politifact_fake_users, base_dir +'/Filtered_date_users_with_three_tweets_or_more/features_politifact_fake_users.pkl')\n",
        "calc_features(politifact_real_users, base_dir +'/Filtered_date_users_with_three_tweets_or_more/features_politifact_real_users.pkl')\n",
        "calc_features(gossipcop_fake_users, base_dir +'/Filtered_date_users_with_three_tweets_or_more/features_gossipcop_fake_users.pkl')\n",
        "calc_features(gossipcop_real_users, base_dir +'/Filtered_date_users_with_three_tweets_or_more/features_gossipcop_real_users.pkl')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 6322/6322 [05:06<00:00, 20.65it/s]\n",
            "100%|██████████| 12571/12571 [12:33<00:00, 16.68it/s]\n",
            "100%|██████████| 29384/29384 [10:43<00:00, 45.66it/s] \n",
            "100%|██████████| 2831/2831 [02:52<00:00, 16.39it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ICECaoVoiBZ"
      },
      "source": [
        "T-Test Calculation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "id": "hcv5XUuJl6rH",
        "outputId": "cbc15dfd-86c0-4b36-f0b3-f65e1f998b1f"
      },
      "source": [
        "from scipy import stats\n",
        "def t_test(x, y):\n",
        "    print (stats.ttest_ind(x, y, nan_policy='omit'))\n",
        "\n",
        "features_politifact_fake_users = pickle.load(open(base_dir +'/Filtered_date_users_with_three_tweets_or_more/features_politifact_fake_users.pkl', 'rb'))\n",
        "features_politifact_fake_users = features_politifact_fake_users.values()\n",
        "temp = list(features_politifact_fake_users)[0]\n",
        "print (temp.keys())\n",
        "\n",
        "features_politifact_fake_users = [np.array(list(x.values())).astype(float) for x in features_politifact_fake_users]\n",
        "\n",
        "features_politifact_real_users = pickle.load(open(base_dir +'/Filtered_date_users_with_three_tweets_or_more/features_politifact_real_users.pkl', 'rb'))\n",
        "features_politifact_real_users = features_politifact_real_users.values()\n",
        "features_politifact_real_users = [np.array(list(x.values())).astype(float) for x in features_politifact_real_users]\n",
        "\n",
        "t_test(features_politifact_fake_users, features_politifact_real_users)\n",
        "\n",
        "\n",
        "features_gossipcop_fake_users = pickle.load(open(base_dir +'/Filtered_date_users_with_three_tweets_or_more/features_gossipcop_fake_users.pkl', 'rb'))\n",
        "features_gossipcop_fake_users = features_gossipcop_fake_users.values()\n",
        "temp = list(features_gossipcop_fake_users)[0]\n",
        "print (temp.keys())\n",
        "features_gossipcop_fake_users = [np.array(list(x.values())).astype(float) for x in features_gossipcop_fake_users]\n",
        "\n",
        "features_gossipcop_real_users = pickle.load(open(base_dir +'/Filtered_date_users_with_three_tweets_or_more/features_gossipcop_real_users.pkl', 'rb'))\n",
        "features_gossipcop_real_users = features_gossipcop_real_users.values()\n",
        "features_gossipcop_real_users = [np.array(list(x.values())).astype(float) for x in features_gossipcop_real_users]\n",
        "\n",
        "t_test(features_gossipcop_fake_users, features_gossipcop_real_users)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['anx', 'discrep', 'tentat', 'certain', 'swear', 'future', 'friends', 'followers', 'avg_twt', 'rel_enhance_retweet', 'rel_enhance_favorite'])\n",
            "Ttest_indResult(statistic=masked_array(data=[-1.060969647424242, -16.131825477360145,\n",
            "                   -22.911369348879962, -8.67267658691202,\n",
            "                   -4.438442709285443, -14.660629355317466,\n",
            "                   -0.26808379735538895, -2.6961300760615283,\n",
            "                   -13.129227322121329, -3.7690750637307047,\n",
            "                   -1.54443835428257],\n",
            "             mask=[False, False, False, False, False, False, False, False,\n",
            "                   False, False, False],\n",
            "       fill_value=1e+20), pvalue=masked_array(data=[2.88717374e-001, 3.75856225e-058, 1.34157328e-114,\n",
            "                   4.55995699e-018, 9.11292927e-006, 2.13620577e-048,\n",
            "                   7.88637759e-001, 7.02127181e-003, 3.33409269e-039,\n",
            "                   1.64355346e-004, 1.22499038e-001],\n",
            "             mask=False,\n",
            "       fill_value=1e+20))\n",
            "dict_keys(['anx', 'discrep', 'tentat', 'certain', 'swear', 'future', 'friends', 'followers', 'avg_twt', 'rel_enhance_retweet', 'rel_enhance_favorite'])\n",
            "Ttest_indResult(statistic=masked_array(data=[1.0229309867296097, 16.43589891559995,\n",
            "                   17.800037928953255, 16.361757591790575,\n",
            "                   13.870727467118366, 19.180827779700454,\n",
            "                   -4.194699045486112, -2.3079783040534263,\n",
            "                   -0.41926032320284473, 9.700637967889637,\n",
            "                   -0.7020939370817902],\n",
            "             mask=[False, False, False, False, False, False, False, False,\n",
            "                   False, False, False],\n",
            "       fill_value=1e+20), pvalue=masked_array(data=[3.06357757e-01, 3.75964372e-60, 4.01583159e-70,\n",
            "                   1.24553217e-59, 1.82044463e-43, 5.53385615e-81,\n",
            "                   2.74852926e-05, 2.10142980e-02, 6.75032176e-01,\n",
            "                   3.50593660e-22, 4.82631989e-01],\n",
            "             mask=False,\n",
            "       fill_value=1e+20))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MO6y6ru9l6rH",
        "outputId": "e3321995-178e-4c48-96dc-2775d1b1ce35"
      },
      "source": [
        "def read_news_users(dir):\n",
        "    vectors = {}\n",
        "    text = []\n",
        "    articles = os.listdir(dir)\n",
        "    for article in tqdm(articles):\n",
        "        vectors[article] = {'user_ids':set(), 'vector':[]}\n",
        "        if os.path.isdir(dir + '/' + article):\n",
        "            fin = open(dir + '/' + article + '/tweets.json')\n",
        "            tweets = json.loads(fin.readline(), strict=False)['tweets']\n",
        "            for tweet in tweets:\n",
        "                user_id = str(tweet['user_id'])\n",
        "                vectors[article]['user_ids'].add(user_id)\n",
        "    return vectors #'article_id':{'user_ids':[], 'vector':[]}\n",
        "\n",
        "politifact_real_news_vectors = read_news_users(base_dir + 'politifact_real')\n",
        "politifact_fake_news_vectors = read_news_users(base_dir + 'politifact_fake')\n",
        "gossipcop_real_news_vectors = read_news_users(base_dir + 'gossipcop_real')\n",
        "gossipcop_fake_news_vectors = read_news_users(base_dir + 'gossipcop_fake')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 624/624 [00:01<00:00, 519.54it/s]\n",
            "100%|██████████| 432/432 [00:00<00:00, 726.40it/s]\n",
            "100%|██████████| 16817/16817 [00:04<00:00, 4000.19it/s]\n",
            "100%|██████████| 6048/6048 [00:03<00:00, 1891.93it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaFvrMkzl6rH"
      },
      "source": [
        "anx_cat=128\n",
        "discrep_cat=134\n",
        "tentat_cat=135\n",
        "certain_cat=136\n",
        "swear_cat=22\n",
        "future_cat=15\n",
        "\n",
        "# print (liwc_analysis(a, anx_words))\n",
        "def calc_features(users1, users2):\n",
        "    anx_words = load_liwc(base_dir +'LIWC2007_Words.txt', anx_cat)\n",
        "    discrep_words = load_liwc(base_dir +'LIWC2007_Words.txt', discrep_cat)\n",
        "    tentat_words = load_liwc(base_dir +'LIWC2007_Words.txt', tentat_cat)\n",
        "    certain_words = load_liwc(base_dir +'LIWC2007_Words.txt', certain_cat)\n",
        "    swear_words = load_liwc(base_dir +'LIWC2007_Words.txt', swear_cat)\n",
        "    future_words = load_liwc(base_dir +'LIWC2007_Words.txt', future_cat)\n",
        "    for k in users1:\n",
        "        twt = users1[k]['tweets']\n",
        "        users1[k] = {}\n",
        "        users1[k]['anx'] = liwc_analysis(twt, anx_words)\n",
        "        users1[k]['discrep'] = liwc_analysis(twt, discrep_words)\n",
        "        users1[k]['tentat'] = liwc_analysis(twt, tentat_words)\n",
        "        users1[k]['certain'] = liwc_analysis(twt, certain_words)\n",
        "        users1[k]['swear'] = liwc_analysis(twt, swear_words)\n",
        "        users1[k]['future'] = liwc_analysis(twt, future_words)\n",
        "    for k in users2:\n",
        "        twt = users2[k]\n",
        "        users2[k] = {}\n",
        "        users2[k]['anx'] = liwc_analysis(twt, anx_words)\n",
        "        users2[k]['discrep'] = liwc_analysis(twt, discrep_words)\n",
        "        users2[k]['tentat'] = liwc_analysis(twt, tentat_words)\n",
        "        users2[k]['certain'] = liwc_analysis(twt, certain_words)\n",
        "        users2[k]['swear'] = liwc_analysis(twt, swear_words)\n",
        "        users2[k]['future'] = liwc_analysis(twt, future_words)\n",
        "    return users1, users2\n",
        "        \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpM0YYtjpBnG"
      },
      "source": [
        "BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IwrP-rElflC"
      },
      "source": [
        "# Neural Net\n",
        "\n",
        "try:\n",
        "    import transformers\n",
        "except:\n",
        "    !pip install transformers\n",
        "    import transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQLxHUWtlflC",
        "outputId": "635a6d44-af77-4d6c-8db6-a40acb21971d"
      },
      "source": [
        "!python3 -m spacy download en"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.3.1 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz#egg=en_core_web_sm==2.3.1 in /home/local/ASUAD/mkarami/anaconda3/lib/python3.8/site-packages (2.3.1)\n",
            "Requirement already satisfied: spacy<2.4.0,>=2.3.0 in /home/local/ASUAD/mkarami/.local/lib/python3.8/site-packages (from en_core_web_sm==2.3.1) (2.3.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/local/ASUAD/mkarami/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/local/ASUAD/mkarami/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.24.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /home/local/ASUAD/mkarami/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/local/ASUAD/mkarami/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/local/ASUAD/mkarami/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.3)\n",
            "Requirement already satisfied: setuptools in /home/local/ASUAD/mkarami/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (49.2.0.post20200714)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /home/local/ASUAD/mkarami/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.19.2)\n",
            "Requirement already satisfied: thinc==7.4.1 in /home/local/ASUAD/mkarami/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /home/local/ASUAD/mkarami/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.8.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/local/ASUAD/mkarami/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.49.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /home/local/ASUAD/mkarami/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /home/local/ASUAD/mkarami/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /home/local/ASUAD/mkarami/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.4.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/local/ASUAD/mkarami/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.25.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /home/local/ASUAD/mkarami/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /home/local/ASUAD/mkarami/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/local/ASUAD/mkarami/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2020.6.20)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/home/local/ASUAD/mkarami/anaconda3/lib/python3.8/site-packages/en_core_web_sm\n",
            "--> /home/local/ASUAD/mkarami/.local/lib/python3.8/site-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqNV-YcRlflD",
        "outputId": "ce95cba9-03ab-4d34-a65a-6d2e3332b8fa"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /home/local/ASUAD/mkarami/anaconda3/lib/python3.8/site-packages (4.4.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/local/ASUAD/mkarami/.local/lib/python3.8/site-packages (from transformers) (2020.9.27)\n",
            "Requirement already satisfied: tqdm>=4.27 in /home/local/ASUAD/mkarami/.local/lib/python3.8/site-packages (from transformers) (4.49.0)\n",
            "Requirement already satisfied: filelock in /home/local/ASUAD/mkarami/.local/lib/python3.8/site-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /home/local/ASUAD/mkarami/.local/lib/python3.8/site-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/local/ASUAD/mkarami/anaconda3/lib/python3.8/site-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: requests in /home/local/ASUAD/mkarami/anaconda3/lib/python3.8/site-packages (from transformers) (2.24.0)\n",
            "Requirement already satisfied: packaging in /home/local/ASUAD/mkarami/.local/lib/python3.8/site-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/local/ASUAD/mkarami/.local/lib/python3.8/site-packages (from transformers) (1.19.2)\n",
            "Requirement already satisfied: joblib in /home/local/ASUAD/mkarami/.local/lib/python3.8/site-packages (from sacremoses->transformers) (0.17.0)\n",
            "Requirement already satisfied: click in /home/local/ASUAD/mkarami/anaconda3/lib/python3.8/site-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /home/local/ASUAD/mkarami/anaconda3/lib/python3.8/site-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/local/ASUAD/mkarami/anaconda3/lib/python3.8/site-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /home/local/ASUAD/mkarami/anaconda3/lib/python3.8/site-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/local/ASUAD/mkarami/anaconda3/lib/python3.8/site-packages (from requests->transformers) (1.25.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /home/local/ASUAD/mkarami/anaconda3/lib/python3.8/site-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /home/local/ASUAD/mkarami/.local/lib/python3.8/site-packages (from packaging->transformers) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgxmKF7-lflD",
        "outputId": "4d16e1b7-307d-4603-f6be-d9451c77dcc8"
      },
      "source": [
        "print(len(politifact_fake_users))\n",
        "print(len(politifact_real_users))\n",
        "print(len(gossipcop_fake_users))\n",
        "print(len(gossipcop_real_users))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6322\n",
            "12571\n",
            "29384\n",
            "2831\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFI5Lj2-lflD",
        "outputId": "d8a68a42-b441-4a0f-9a95-3bf4b27ae6a4"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHXWcNndlflD"
      },
      "source": [
        "import re\n",
        "emoji_pattern = re.compile(\"[\"\n",
        "         u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "         u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "         u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "         u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "         u\"\\U00002702-\\U000027B0\"\n",
        "         u\"\\U000024C2-\\U0001F251\"\n",
        "         \"]+\", flags=re.UNICODE)\n",
        "\n",
        "emoticons = set([\n",
        "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
        "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
        "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
        "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
        "    '<3', ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
        "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
        "    ':c', ':{', '>:\\\\', ';('\n",
        "    ])\n",
        "\n",
        "def clean_tweets(tweet, rm_puncs=True):\n",
        "    tweet = re.sub(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\", \"\", tweet)\n",
        "    word_tokens = tweet.replace('\"', '').replace('<br />', '').replace(')', '').replace('(', '').lower().split(' ')\n",
        "    \n",
        "    #after tweepy preprocessing the colon symbol left remain after      #removing mentions\n",
        "    tweet = re.sub(r':', '', tweet)\n",
        "    tweet = re.sub(r'‚Ä¶', '', tweet)\n",
        "    \n",
        "    #replace consecutive non-ASCII characters with a space\n",
        "    tweet = re.sub(r'[^\\x00-\\x7F]+',' ', tweet)\n",
        "    \n",
        "    #remove emojis from tweet\n",
        "    tweet = emoji_pattern.sub(r'', tweet)\n",
        "    \n",
        "    #looping through conditions\n",
        "    filtered_tweet = []\n",
        "    \n",
        "    for w in word_tokens:\n",
        "        #check tokens against stop words , emoticons and punctuations\n",
        "        if (w not in emoticons and not rm_puncs) or (rm_puncs and w not in string.punctuation and w not in emoticons):\n",
        "            filtered_tweet.append(w)\n",
        "    \n",
        "    text = ' '.join(filtered_tweet)\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fzw0j0BvlflD"
      },
      "source": [
        "pickle.dump(politifact_fake_users, open(base_dir+'cache/'+'politifact_fake_users', 'wb'))\n",
        "pickle.dump(politifact_real_users, open(base_dir+'cache/'+'politifact_real_users', 'wb'))\n",
        "pickle.dump(gossipcop_fake_users, open(base_dir+'cache/'+'gossipcop_fake_users', 'wb'))\n",
        "pickle.dump(gossipcop_real_users, open(base_dir+'cache/'+'gossipcop_real_users', 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irJNWRV3lflD",
        "outputId": "3d1838d4-a1e6-41ab-c816-5c687c204f18"
      },
      "source": [
        "politifact_users_X = []\n",
        "politifact_users_y = []\n",
        "gossipcop_users_X = []\n",
        "gossipcop_users_y = []\n",
        "\n",
        "gossipcop_users = {}\n",
        "for users in tqdm(politifact_fake_users):\n",
        "    if politifact_fake_users[users]['timeline'] is not None:\n",
        "        politifact_users_X.append(clean_tweets(' '.join(politifact_fake_users[users]['timeline'])))\n",
        "        politifact_users_y.append(1)\n",
        "    else:\n",
        "        politifact_users_X.append(\"\")\n",
        "        politifact_users_y.append(1)\n",
        "        \n",
        "for users in tqdm(politifact_real_users):\n",
        "    if politifact_real_users[users]['timeline'] is not None:\n",
        "        politifact_users_X.append(clean_tweets(' '.join(politifact_real_users[users]['timeline'])))\n",
        "        politifact_users_y.append(0)\n",
        "    else:\n",
        "        politifact_users_X.append(\"\")\n",
        "        politifact_users_y.append(0)\n",
        "    \n",
        "for users in tqdm(gossipcop_fake_users):\n",
        "    if gossipcop_fake_users[users]['timeline'] is not None:\n",
        "        gossipcop_users_X.append(clean_tweets(' '.join(gossipcop_fake_users[users]['timeline'])))\n",
        "        gossipcop_users_y.append(1)\n",
        "    else:\n",
        "        gossipcop_users_X.append(\"\")\n",
        "        gossipcop_users_y.append(1)\n",
        "        \n",
        "for users in tqdm(gossipcop_real_users):\n",
        "    if gossipcop_real_users[users]['timeline'] is not None:\n",
        "        gossipcop_users_X.append(clean_tweets(' '.join(gossipcop_real_users[users]['timeline'])))\n",
        "        gossipcop_users_y.append(0)\n",
        "    else:\n",
        "        gossipcop_users_X.append(\"\")\n",
        "        gossipcop_users_y.append(0)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 6322/6322 [00:07<00:00, 835.72it/s] \n",
            "100%|██████████| 12571/12571 [00:19<00:00, 641.11it/s]\n",
            "100%|██████████| 29384/29384 [00:17<00:00, 1672.77it/s]\n",
            "100%|██████████| 2831/2831 [00:04<00:00, 642.94it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3W3gpihlflE",
        "outputId": "f78a6023-050c-4d8c-a3d5-23ae150151c6"
      },
      "source": [
        "print(len(politifact_users_X))\n",
        "print(len(gossipcop_users_X))\n",
        "\n",
        "print(len(politifact_users_y))\n",
        "print(len(gossipcop_users_y))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "18893\n",
            "32215\n",
            "18893\n",
            "32215\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNo3nP_xlflE"
      },
      "source": [
        "pickle.dump(politifact_users_X, open(base_dir+'cache/'+'politifact_X', 'wb'))\n",
        "pickle.dump(politifact_users_y, open(base_dir+'cache/'+'politifact_y', 'wb'))\n",
        "pickle.dump(gossipcop_users_X, open(base_dir+'cache/'+'gossipcop_X', 'wb'))\n",
        "pickle.dump(gossipcop_users_y, open(base_dir+'cache/'+'gossipcop_y', 'wb'))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8Di0H0HlflE",
        "outputId": "536c2670-e62d-491a-9ac0-4c6d70252a55"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YERYTypKlflE"
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "import torch\n",
        "def tweet_encoding(X_data, y_data):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "#     count = 0\n",
        "    # For every sentence...\n",
        "    for sent in tqdm(X_data):\n",
        "        # `encode_plus` will:\n",
        "        #   (1) Tokenize the sentence.\n",
        "        #   (2) Prepend the `[CLS]` token to the start.\n",
        "        #   (3) Append the `[SEP]` token to the end.\n",
        "        #   (4) Map tokens to their IDs.\n",
        "        #   (5) Pad or truncate the sentence to `max_length`\n",
        "        #   (6) Create attention masks for [PAD] tokens.\n",
        "        encoded_dict = tokenizer.encode_plus(\n",
        "                            sent,                      # Sentence to encode.\n",
        "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                            max_length = 256,           # Pad & truncate all sentences.\n",
        "#                             padding = True,\n",
        "#                             truncate = True, \n",
        "                            pad_to_max_length = True,\n",
        "                            return_attention_mask = True,   # Construct attn. masks.\n",
        "                            return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                       )\n",
        "\n",
        "        # Add the encoded sentence to the list.    \n",
        "        input_ids.append(encoded_dict['input_ids'])\n",
        "\n",
        "        # And its attention mask (simply differentiates padding from non-padding).\n",
        "        attention_masks.append(encoded_dict['attention_mask'])\n",
        "#         count +=1\n",
        "#         if count == 10: break\n",
        "\n",
        "    # Convert the lists into tensors.\n",
        "    print(len(input_ids))\n",
        "    print(len(attention_masks))\n",
        "    print(len(y_data))\n",
        "    input_ids = torch.cat(input_ids, dim=0)\n",
        "    attention_masks = torch.cat(attention_masks, dim=0)\n",
        "    labels = torch.tensor(y_data)\n",
        "    return input_ids, attention_masks, labels\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgQ_5jPflflE",
        "outputId": "1e0d91cf-527c-4540-9a00-24dc42d0e045"
      },
      "source": [
        "input_ids_politifact_users, attention_masks_politifact_users, labels_politifact_users = tweet_encoding(politifact_users_X, politifact_users_y)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 18893/18893 [15:08<00:00, 20.80it/s] \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "18893\n",
            "18893\n",
            "18893\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIYCSmHblflF",
        "outputId": "c43f784e-2a63-4674-a531-c5a470c277f0"
      },
      "source": [
        "input_ids_gossipcop_users, attention_masks_gossipcop_users, labels_gossipcop_users = tweet_encoding(gossipcop_users_X, gossipcop_users_y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 32215/32215 [10:54<00:00, 49.19it/s] \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "32215\n",
            "32215\n",
            "32215\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZEXpE_ClflF"
      },
      "source": [
        "torch.save(input_ids_politifact_users, base_dir+'cache/'+'input_ids_politifact_users')\n",
        "torch.save(attention_masks_politifact_users, base_dir+'cache/'+'attention_masks_politifact_users')\n",
        "torch.save(labels_politifact_users, base_dir+'cache/'+'labels_politifact_users')\n",
        "\n",
        "torch.save(input_ids_gossipcop_users, base_dir+'cache/'+'input_ids_gossipcop_users')\n",
        "torch.save(attention_masks_gossipcop_users, base_dir+'cache/'+'attention_masks_gossipcop_users')\n",
        "torch.save(labels_gossipcop_users, base_dir+'cache/'+'labels_gossipcop_users')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbkESOjgpWQT"
      },
      "source": [
        "Switch to the training files"
      ]
    }
  ]
}