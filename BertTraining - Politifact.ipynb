{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"colab":{"name":"BertTraining - Politifact - Submitted.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"VcKFR30cqZnC"},"source":["# Politifact"]},{"cell_type":"code","metadata":{"id":"1_wzzbnaqZnE","outputId":"38f18ac1-523d-461c-93b3-69697340debc"},"source":["import os\n","base_dir = os.getcwd() + '/'\n","print('Base Dir:', base_dir)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Base Dir: /home/local/ASUAD/mkarami/Documents/Motivational Factors/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"16QbL9F9qZnH","outputId":"c20a62c8-3993-41c0-ae41-c4c7c4f03c80"},"source":["from transformers import BertTokenizer\n","\n","# Load the BERT tokenizer.\n","print('Loading BERT tokenizer...')\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loading BERT tokenizer...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zlczlffdqZnJ","outputId":"a2ae26ea-90c8-4865-ab65-f862f3712722"},"source":["import torch\n","input_ids_politifact = torch.load(base_dir+'cache/input_ids_politifact_users')\n","attention_masks_politifact = torch.load(base_dir+'cache/attention_masks_politifact_users')\n","labels_politifact = torch.load(base_dir+'cache/labels_politifact_users')\n","\n","print(input_ids_politifact[0])\n","print(attention_masks_politifact.size())\n","print(labels_politifact.size())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([  101,  4938,  8945,  3126, 21351,  2078,  2179,  2757,  1010,  6086,\n","         7207,  2015,  1010, 11417, 15493,  1010,  2784,  2110, 10812,  6086,\n","         1024,  3081, 17328,  2703,  4575,  2074,  3936,  2010,  2784,  2110,\n","         4707,  2007,  6745,  4861,  1024,  3081,  8174,  2231,  2435,  8112,\n","        14895,  2015,  1523, 15940,  2015,  1011,  2440,  1997, 15565,  1524,\n","         1010,  2758,  4654,  1011,  2880,  1024,  3081,  5230, 15742,  3603,\n","         2006,  2455,  4198,  2000,  7207,  3192,  1024,  3081,  9295,  7207,\n","         1056, 28394,  3215,  1005, 10733,  5867,  2003,  2613,  1005,  1024,\n","         3081,  2192, 15773,  8112,  3602,  2179,  2008,  6083,  2002,  2001,\n","         2112,  1997, 18520,  3104,  1011,  2039,  1024,  3081,  2054,  3653,\n","         2015,  8398,  2074,  4197,  2041,  2055,  1996,  6965,  5690,  2097,\n","         2031, 26774,  1521,  1055,  4308,  1999,  9439,  1024,  3081,  1996,\n","         2995,  4767,  1997,  1053,  2019,  2239,  2038,  2042,  2179,  1010,\n","         1001,  7482,  1996,  9932,  2003,  2941,  1053,  1024,  3081,  2633,\n","         1024,  8495,  1521,  1055, 18520, 10373,  9446,  2415,  1997,  9046,\n","         4001,  4994,  1024,  3081,  6221,  8398,  1024,  4001, 17183,  2015,\n","         1520, 22363,  1996,  2097,  1997,  1996,  2137, 14303,  1010,  1521,\n","        26478,  6072, 19301, 17183,  2015,  1520,  2123,  1521,  1056,  2729,\n","         1024,  9767, 18223, 11449,  3065,  2845,  8645,  9058,  2587,  2007,\n","         2569,  9517,  2728, 26774,  1024,  3081, 22017,  5358,  1012,  1012,\n","         6545,  1997,  2317,  2160,  3639,  2880,  1024,  3081,  5736, 23288,\n","         2179,  2757,  1010,  7207,  3192,  2499,  2007,  5859,  1999, 12867,\n","         1024,  3081,  3189,  1024,  2065, 26774,  3138,  2006,  8962,  2271,\n","         1010,  6221,  1521,  1055,  3423,  2136,  2097,  2377,  2049,  8398,\n","         4003,  1024,  3081, 17727,  5243, 22729,  6739,  3021,  7207, 21094,\n","         1999,  2006,  4447,  8398,  2323,   102])\n","torch.Size([18893, 256])\n","torch.Size([18893])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NX2WkvzAqZnK","outputId":"f87d339c-dcb6-4648-c83a-21c889500cef"},"source":["from torch.utils.data import TensorDataset, random_split\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","\n","# Combine the training inputs into a TensorDataset.\n","dataset = TensorDataset(input_ids_politifact, attention_masks_politifact, labels_politifact)\n","\n","# Create a 90-10 train-validation split.\n","\n","# Calculate the number of samples to include in each set.\n","train_size = int(0.9 * len(dataset))\n","val_size = len(dataset) - train_size\n","\n","# Divide the dataset by randomly selecting samples.\n","train_dataset, valid_dataset = random_split(dataset, [train_size, val_size])\n","\n","print('{:>5,} training samples'.format(train_size))\n","print('{:>5,} validation samples'.format(val_size))\n","\n","\n","# The DataLoader needs to know our batch size for training, so we specify it \n","# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n","# size of 16 or 32.\n","batch_size = 8\n","\n","# Create the DataLoaders for our training and validation sets.\n","# We'll take training samples in random order. \n","train_loader = DataLoader(\n","            train_dataset,  # The training samples.\n","            sampler = RandomSampler(train_dataset), # Select batches randomly\n","            batch_size = batch_size # Trains with this batch size.\n","        )\n","\n","\n","# For validation the order doesn't matter, so we'll just read them sequentially.\n","valid_loader = DataLoader(\n","            valid_dataset, # The validation samples.\n","            sampler = SequentialSampler(valid_dataset), # Pull out batches sequentially.\n","            batch_size = batch_size # Evaluate with this batch size.\n","        )\n","\n","\n","\n","print('Train = %d (%d) | Validation = %d (%d)' %(len(train_dataset), len(train_loader), len(valid_dataset), len(valid_loader)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["17,003 training samples\n","1,890 validation samples\n","Train = 17003 (2126) | Validation = 1890 (237)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QlPbDTHBqZnL","outputId":"b9a69ab2-c63d-4c92-a006-ae971eceb59f"},"source":["from transformers import BertForSequenceClassification, AdamW, BertConfig\n","\n","# Load BertForSequenceClassification, the pretrained BERT model with a single \n","# linear classification layer on top. \n","model = BertForSequenceClassification.from_pretrained(\n","    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n","    num_labels = 2, # The number of output labels--2 for binary classification.\n","                    # You can increase this for multi-class tasks.   \n","    output_attentions = False, # Whether the model returns attentions weights.\n","    output_hidden_states = True, # Whether the model returns all hidden-states.\n",")\n","\n","# Tell pytorch to run this model on the GPU.\n","model.cuda()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"-QODG2DMqZnL"},"source":["# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n","# I believe the 'W' stands for 'Weight Decay fix\"\n","optimizer = AdamW(model.parameters(),\n","                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n","                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n","                )\n","\n","\n","from transformers import get_linear_schedule_with_warmup\n","\n","# Number of training epochs. The BERT authors recommend between 2 and 4. \n","# We chose to run for 4, but we'll see later that this may be over-fitting the\n","# training data.\n","epochs = 4\n","\n","# Total number of training steps is [number of batches] x [number of epochs]. \n","# (Note that this is not the same as the number of training samples).\n","total_steps = len(train_loader) * epochs\n","\n","# Create the learning rate scheduler.\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 0, # Default value in run_glue.py\n","                                            num_training_steps = total_steps)\n","\n","\n","import numpy as np\n","\n","# Function to calculate the accuracy of our predictions vs labels\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n","\n","\n","import time\n","import datetime\n","\n","def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vMIqoFSlqZnM","outputId":"f6577a92-95cb-4652-f613-82dbc5ee16bc"},"source":["import torch\n","\n","# If there's a GPU available...\n","if torch.cuda.is_available():    \n","\n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","    model = model.to(device)\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["There are 2 GPU(s) available.\n","We will use the GPU: GeForce RTX 2080 Ti\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FeqMRD6TqZnM"},"source":["from sklearn.metrics import recall_score\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import f1_score\n","\n","def flat_precision(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return recall_score(labels_flat, pred_flat)\n","\n","def flat_recall(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return precision_score(labels_flat, pred_flat)\n","\n","def flat_f1_score(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return f1_score(labels_flat, pred_flat)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mWEVq_dpqZnN","outputId":"d25304cb-c24e-4370-ac97-48c00d93f9d0"},"source":["import random\n","import numpy as np\n","from tqdm import tqdm\n","\n","# This training code is based on the `run_glue.py` script here:\n","# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n","\n","# Set the seed value all over the place to make this reproducible.\n","seed_val = 42\n","\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","# We'll store a number of quantities such as training and validation loss, \n","# validation accuracy, and timings.\n","training_stats = []\n","\n","# Measure the total training time for the whole run.\n","total_t0 = time.time()\n","\n","sentence_embedding_train = []\n","sentence_embedding_val = []\n","\n","\n","# For each epoch...\n","for epoch_i in range(0, epochs):\n","    \n","    # ========================================\n","    #               Training\n","    # ========================================\n","    \n","    # Perform one full pass over the training set.\n","\n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","\n","    # Measure how long the training epoch takes.\n","    t0 = time.time()\n","\n","    # Reset the total loss for this epoch.\n","    total_train_loss = 0\n","\n","    # Put the model into training mode. Don't be mislead--the call to \n","    # `train` just changes the *mode*, it doesn't *perform* the training.\n","    # `dropout` and `batchnorm` layers behave differently during training\n","    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n","    model.train()\n","\n","    # For each batch of training data...\n","    for step, batch in enumerate(train_loader):\n","\n","        # Progress update every 40 batches.\n","        if step % 50 == 0 and not step == 0:\n","            # Calculate elapsed time in minutes.\n","            elapsed = format_time(time.time() - t0)\n","            \n","            # Report progress.\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_loader), elapsed))\n","\n","        # Unpack this training batch from our dataloader. \n","        #\n","        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n","        # `to` method.\n","        #\n","        # `batch` contains three pytorch tensors:\n","        #   [0]: input ids \n","        #   [1]: attention masks\n","        #   [2]: labels \n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","\n","        # Always clear any previously calculated gradients before performing a\n","        # backward pass. PyTorch doesn't do this automatically because \n","        # accumulating the gradients is \"convenient while training RNNs\". \n","        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n","        model.zero_grad()        \n","\n","        # Perform a forward pass (evaluate the model on this training batch).\n","        # The documentation for this `model` function is here: \n","        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","        # It returns different numbers of parameters depending on what arguments\n","        # are given and what flags are set. For our useage here, it returns\n","        # the loss (because we provided labels) and the \"logits\"--the model\n","        # outputs prior to activation.\n","        loss, logits, hidden_states_train = model(b_input_ids, \n","                             token_type_ids=None, \n","                             attention_mask=b_input_mask, \n","                             labels=b_labels)\n","        \n","#         token_vecs = hidden_states_train[-2][0]\n","\n","        # Calculate the average of all 22 token vectors.\n","#         sentence_embedding_train.append(torch.mean(token_vecs, dim=0))\n","        \n","        \n","\n","        # Accumulate the training loss over all of the batches so that we can\n","        # calculate the average loss at the end. `loss` is a Tensor containing a\n","        # single value; the `.item()` function just returns the Python value \n","        # from the tensor.\n","        total_train_loss += loss.item()\n","\n","        # Perform a backward pass to calculate the gradients.\n","        loss.backward()\n","\n","        # Clip the norm of the gradients to 1.0.\n","        # This is to help prevent the \"exploding gradients\" problem.\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        # Update parameters and take a step using the computed gradient.\n","        # The optimizer dictates the \"update rule\"--how the parameters are\n","        # modified based on their gradients, the learning rate, etc.\n","        optimizer.step()\n","\n","        # Update the learning rate.\n","        scheduler.step()\n","    \n","    \n","\n","    # Calculate the average loss over all of the batches.\n","    avg_train_loss = total_train_loss / len(train_loader)            \n","    \n","    # Measure how long this epoch took.\n","    training_time = format_time(time.time() - t0)\n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epcoh took: {:}\".format(training_time))\n","        \n","    # ========================================\n","    #               Validation\n","    # ========================================\n","    # After the completion of each training epoch, measure our performance on\n","    # our validation set.\n","\n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    t0 = time.time()\n","\n","    # Put the model in evaluation mode--the dropout layers behave differently\n","    # during evaluation.\n","    model.eval()\n","\n","    # Tracking variables \n","    total_eval_accuracy = 0\n","    total_eval_precision = 0\n","    total_eval_recall = 0\n","    total_eval_f1 = 0\n","    total_eval_loss = 0\n","    nb_eval_steps = 0\n","\n","    # Evaluate data for one epoch\n","    for batch in valid_loader:\n","        \n","        # Unpack this training batch from our dataloader. \n","        #\n","        # As we unpack the batch, we'll also copy each tensor to the GPU using \n","        # the `to` method.\n","        #\n","        # `batch` contains three pytorch tensors:\n","        #   [0]: input ids \n","        #   [1]: attention masks\n","        #   [2]: labels \n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","        \n","        # Tell pytorch not to bother with constructing the compute graph during\n","        # the forward pass, since this is only needed for backprop (training).\n","        with torch.no_grad():        \n","\n","            # Forward pass, calculate logit predictions.\n","            # token_type_ids is the same as the \"segment ids\", which \n","            # differentiates sentence 1 and 2 in 2-sentence tasks.\n","            # The documentation for this `model` function is here: \n","            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","            # Get the \"logits\" output by the model. The \"logits\" are the output\n","            # values prior to applying an activation function like the softmax.\n","            (loss, logits, hidden_states_val) = model(b_input_ids, \n","                                   token_type_ids=None, \n","                                   attention_mask=b_input_mask,\n","                                   labels=b_labels)\n","        \n","#         token_vecs = hidden_states_val[-2][0]\n","            # Calculate the average of all 22 token vectors.\n","#         sentence_embedding_val.append(torch.mean(token_vecs, dim=0))\n","            \n","        # Accumulate the validation loss.\n","        total_eval_loss += loss.item()\n","\n","        # Move logits and labels to CPU\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","\n","        # Calculate the accuracy for this batch of test sentences, and\n","        # accumulate it over all batches.\n","        total_eval_accuracy += flat_accuracy(logits, label_ids)\n","        total_eval_precision += flat_precision(logits, label_ids)\n","        total_eval_recall += flat_recall(logits, label_ids)\n","        total_eval_f1 += flat_f1_score(logits, label_ids)\n","    \n","       \n","\n","    # Report the final accuracy for this validation run.\n","    avg_val_accuracy = total_eval_accuracy / len(valid_loader)\n","    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n","    \n","    avg_val_precision = total_eval_precision / len(valid_loader)\n","    avg_val_recall = total_eval_recall / len(valid_loader)\n","    avg_val_f1 = total_eval_f1 / len(valid_loader)\n","\n","    # Calculate the average loss over all of the batches.\n","    avg_val_loss = total_eval_loss / len(valid_loader)\n","    \n","    # Measure how long the validation run took.\n","    validation_time = format_time(time.time() - t0)\n","    \n","    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n","    print(\"  Validation took: {:}\".format(validation_time))\n","\n","    # Record all statistics from this epoch.\n","    training_stats.append(\n","        {\n","            'epoch': epoch_i + 1,\n","            'Training Loss': avg_train_loss,\n","            'Valid. Loss': avg_val_loss,\n","            'Valid. Accur.': avg_val_accuracy,\n","            'Training Time': training_time,\n","            'Validation Time': validation_time,\n","            'Valid. Prec': avg_val_precision,\n","            'Valid. Recall': avg_val_recall,\n","            'Valid. F1': avg_val_f1,\n","        }\n","    )\n","\n","print(\"\")\n","print(\"Training complete!\")\n","\n","print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","======== Epoch 1 / 4 ========\n","Training...\n","  Batch    50  of  2,126.    Elapsed: 0:00:09.\n","  Batch   100  of  2,126.    Elapsed: 0:00:18.\n","  Batch   150  of  2,126.    Elapsed: 0:00:27.\n","  Batch   200  of  2,126.    Elapsed: 0:00:36.\n","  Batch   250  of  2,126.    Elapsed: 0:00:46.\n","  Batch   300  of  2,126.    Elapsed: 0:00:55.\n","  Batch   350  of  2,126.    Elapsed: 0:01:04.\n","  Batch   400  of  2,126.    Elapsed: 0:01:14.\n","  Batch   450  of  2,126.    Elapsed: 0:01:23.\n","  Batch   500  of  2,126.    Elapsed: 0:01:32.\n","  Batch   550  of  2,126.    Elapsed: 0:01:42.\n","  Batch   600  of  2,126.    Elapsed: 0:01:51.\n","  Batch   650  of  2,126.    Elapsed: 0:02:00.\n","  Batch   700  of  2,126.    Elapsed: 0:02:10.\n","  Batch   750  of  2,126.    Elapsed: 0:02:19.\n","  Batch   800  of  2,126.    Elapsed: 0:02:28.\n","  Batch   850  of  2,126.    Elapsed: 0:02:38.\n","  Batch   900  of  2,126.    Elapsed: 0:02:47.\n","  Batch   950  of  2,126.    Elapsed: 0:02:57.\n","  Batch 1,000  of  2,126.    Elapsed: 0:03:06.\n","  Batch 1,050  of  2,126.    Elapsed: 0:03:15.\n","  Batch 1,100  of  2,126.    Elapsed: 0:03:24.\n","  Batch 1,150  of  2,126.    Elapsed: 0:03:34.\n","  Batch 1,200  of  2,126.    Elapsed: 0:03:43.\n","  Batch 1,250  of  2,126.    Elapsed: 0:03:53.\n","  Batch 1,300  of  2,126.    Elapsed: 0:04:02.\n","  Batch 1,350  of  2,126.    Elapsed: 0:04:12.\n","  Batch 1,400  of  2,126.    Elapsed: 0:04:21.\n","  Batch 1,450  of  2,126.    Elapsed: 0:04:30.\n","  Batch 1,500  of  2,126.    Elapsed: 0:04:40.\n","  Batch 1,550  of  2,126.    Elapsed: 0:04:49.\n","  Batch 1,600  of  2,126.    Elapsed: 0:04:59.\n","  Batch 1,650  of  2,126.    Elapsed: 0:05:08.\n","  Batch 1,700  of  2,126.    Elapsed: 0:05:18.\n","  Batch 1,750  of  2,126.    Elapsed: 0:05:27.\n","  Batch 1,800  of  2,126.    Elapsed: 0:05:37.\n","  Batch 1,850  of  2,126.    Elapsed: 0:05:46.\n","  Batch 1,900  of  2,126.    Elapsed: 0:05:55.\n","  Batch 1,950  of  2,126.    Elapsed: 0:06:05.\n","  Batch 2,000  of  2,126.    Elapsed: 0:06:14.\n","  Batch 2,050  of  2,126.    Elapsed: 0:06:23.\n","  Batch 2,100  of  2,126.    Elapsed: 0:06:33.\n","\n","  Average training loss: 0.42\n","  Training epcoh took: 0:06:37\n","\n","Running Validation...\n"],"name":"stdout"},{"output_type":"stream","text":["/home/local/ASUAD/mkarami/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/home/local/ASUAD/mkarami/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/home/local/ASUAD/mkarami/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1464: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(\n"],"name":"stderr"},{"output_type":"stream","text":["  Accuracy: 0.79\n","  Validation Loss: 0.58\n","  Validation took: 0:00:13\n","\n","======== Epoch 2 / 4 ========\n","Training...\n","  Batch    50  of  2,126.    Elapsed: 0:00:09.\n","  Batch   100  of  2,126.    Elapsed: 0:00:19.\n","  Batch   150  of  2,126.    Elapsed: 0:00:28.\n","  Batch   200  of  2,126.    Elapsed: 0:00:37.\n","  Batch   250  of  2,126.    Elapsed: 0:00:47.\n","  Batch   300  of  2,126.    Elapsed: 0:00:56.\n","  Batch   350  of  2,126.    Elapsed: 0:01:06.\n","  Batch   400  of  2,126.    Elapsed: 0:01:15.\n","  Batch   450  of  2,126.    Elapsed: 0:01:24.\n","  Batch   500  of  2,126.    Elapsed: 0:01:33.\n","  Batch   550  of  2,126.    Elapsed: 0:01:43.\n","  Batch   600  of  2,126.    Elapsed: 0:01:52.\n","  Batch   650  of  2,126.    Elapsed: 0:02:01.\n","  Batch   700  of  2,126.    Elapsed: 0:02:10.\n","  Batch   750  of  2,126.    Elapsed: 0:02:20.\n","  Batch   800  of  2,126.    Elapsed: 0:02:29.\n","  Batch   850  of  2,126.    Elapsed: 0:02:38.\n","  Batch   900  of  2,126.    Elapsed: 0:02:48.\n","  Batch   950  of  2,126.    Elapsed: 0:02:57.\n","  Batch 1,000  of  2,126.    Elapsed: 0:03:07.\n","  Batch 1,050  of  2,126.    Elapsed: 0:03:16.\n","  Batch 1,100  of  2,126.    Elapsed: 0:03:25.\n","  Batch 1,150  of  2,126.    Elapsed: 0:03:35.\n","  Batch 1,200  of  2,126.    Elapsed: 0:03:44.\n","  Batch 1,250  of  2,126.    Elapsed: 0:03:53.\n","  Batch 1,300  of  2,126.    Elapsed: 0:04:03.\n","  Batch 1,350  of  2,126.    Elapsed: 0:04:12.\n","  Batch 1,400  of  2,126.    Elapsed: 0:04:21.\n","  Batch 1,450  of  2,126.    Elapsed: 0:04:31.\n","  Batch 1,500  of  2,126.    Elapsed: 0:04:40.\n","  Batch 1,550  of  2,126.    Elapsed: 0:04:49.\n","  Batch 1,600  of  2,126.    Elapsed: 0:04:59.\n","  Batch 1,650  of  2,126.    Elapsed: 0:05:08.\n","  Batch 1,700  of  2,126.    Elapsed: 0:05:18.\n","  Batch 1,750  of  2,126.    Elapsed: 0:05:27.\n","  Batch 1,800  of  2,126.    Elapsed: 0:05:36.\n","  Batch 1,850  of  2,126.    Elapsed: 0:05:46.\n","  Batch 1,900  of  2,126.    Elapsed: 0:05:55.\n","  Batch 1,950  of  2,126.    Elapsed: 0:06:04.\n","  Batch 2,000  of  2,126.    Elapsed: 0:06:14.\n","  Batch 2,050  of  2,126.    Elapsed: 0:06:23.\n","  Batch 2,100  of  2,126.    Elapsed: 0:06:33.\n","\n","  Average training loss: 0.36\n","  Training epcoh took: 0:06:38\n","\n","Running Validation...\n","  Accuracy: 0.78\n","  Validation Loss: 0.72\n","  Validation took: 0:00:13\n","\n","======== Epoch 3 / 4 ========\n","Training...\n","  Batch    50  of  2,126.    Elapsed: 0:00:09.\n","  Batch   100  of  2,126.    Elapsed: 0:00:18.\n","  Batch   150  of  2,126.    Elapsed: 0:00:28.\n","  Batch   200  of  2,126.    Elapsed: 0:00:37.\n","  Batch   250  of  2,126.    Elapsed: 0:00:47.\n","  Batch   300  of  2,126.    Elapsed: 0:00:56.\n","  Batch   350  of  2,126.    Elapsed: 0:01:05.\n","  Batch   400  of  2,126.    Elapsed: 0:01:15.\n","  Batch   450  of  2,126.    Elapsed: 0:01:24.\n","  Batch   500  of  2,126.    Elapsed: 0:01:34.\n","  Batch   550  of  2,126.    Elapsed: 0:01:43.\n","  Batch   600  of  2,126.    Elapsed: 0:01:52.\n","  Batch   650  of  2,126.    Elapsed: 0:02:02.\n","  Batch   700  of  2,126.    Elapsed: 0:02:11.\n","  Batch   750  of  2,126.    Elapsed: 0:02:21.\n","  Batch   800  of  2,126.    Elapsed: 0:02:30.\n","  Batch   850  of  2,126.    Elapsed: 0:02:40.\n","  Batch   900  of  2,126.    Elapsed: 0:02:49.\n","  Batch   950  of  2,126.    Elapsed: 0:02:59.\n","  Batch 1,000  of  2,126.    Elapsed: 0:03:08.\n","  Batch 1,050  of  2,126.    Elapsed: 0:03:18.\n","  Batch 1,100  of  2,126.    Elapsed: 0:03:27.\n","  Batch 1,150  of  2,126.    Elapsed: 0:03:36.\n","  Batch 1,200  of  2,126.    Elapsed: 0:03:46.\n","  Batch 1,250  of  2,126.    Elapsed: 0:03:55.\n","  Batch 1,300  of  2,126.    Elapsed: 0:04:04.\n","  Batch 1,350  of  2,126.    Elapsed: 0:04:14.\n","  Batch 1,400  of  2,126.    Elapsed: 0:04:23.\n","  Batch 1,450  of  2,126.    Elapsed: 0:04:33.\n","  Batch 1,500  of  2,126.    Elapsed: 0:04:42.\n","  Batch 1,550  of  2,126.    Elapsed: 0:04:51.\n","  Batch 1,600  of  2,126.    Elapsed: 0:05:01.\n","  Batch 1,650  of  2,126.    Elapsed: 0:05:11.\n","  Batch 1,700  of  2,126.    Elapsed: 0:05:20.\n","  Batch 1,750  of  2,126.    Elapsed: 0:05:29.\n","  Batch 1,800  of  2,126.    Elapsed: 0:05:39.\n","  Batch 1,850  of  2,126.    Elapsed: 0:05:48.\n","  Batch 1,900  of  2,126.    Elapsed: 0:05:58.\n","  Batch 1,950  of  2,126.    Elapsed: 0:06:07.\n","  Batch 2,000  of  2,126.    Elapsed: 0:06:17.\n","  Batch 2,050  of  2,126.    Elapsed: 0:06:26.\n","  Batch 2,100  of  2,126.    Elapsed: 0:06:35.\n","\n","  Average training loss: 0.31\n","  Training epcoh took: 0:06:40\n","\n","Running Validation...\n","  Accuracy: 0.80\n","  Validation Loss: 0.83\n","  Validation took: 0:00:13\n","\n","======== Epoch 4 / 4 ========\n","Training...\n","  Batch    50  of  2,126.    Elapsed: 0:00:09.\n","  Batch   100  of  2,126.    Elapsed: 0:00:19.\n","  Batch   150  of  2,126.    Elapsed: 0:00:28.\n","  Batch   200  of  2,126.    Elapsed: 0:00:38.\n","  Batch   250  of  2,126.    Elapsed: 0:00:47.\n","  Batch   300  of  2,126.    Elapsed: 0:00:56.\n","  Batch   350  of  2,126.    Elapsed: 0:01:05.\n","  Batch   400  of  2,126.    Elapsed: 0:01:15.\n","  Batch   450  of  2,126.    Elapsed: 0:01:24.\n","  Batch   500  of  2,126.    Elapsed: 0:01:34.\n","  Batch   550  of  2,126.    Elapsed: 0:01:43.\n","  Batch   600  of  2,126.    Elapsed: 0:01:53.\n","  Batch   650  of  2,126.    Elapsed: 0:02:02.\n","  Batch   700  of  2,126.    Elapsed: 0:02:11.\n","  Batch   750  of  2,126.    Elapsed: 0:02:21.\n","  Batch   800  of  2,126.    Elapsed: 0:02:30.\n","  Batch   850  of  2,126.    Elapsed: 0:02:40.\n","  Batch   900  of  2,126.    Elapsed: 0:02:49.\n","  Batch   950  of  2,126.    Elapsed: 0:02:58.\n","  Batch 1,000  of  2,126.    Elapsed: 0:03:08.\n","  Batch 1,050  of  2,126.    Elapsed: 0:03:17.\n","  Batch 1,100  of  2,126.    Elapsed: 0:03:26.\n","  Batch 1,150  of  2,126.    Elapsed: 0:03:36.\n","  Batch 1,200  of  2,126.    Elapsed: 0:03:45.\n","  Batch 1,250  of  2,126.    Elapsed: 0:03:55.\n","  Batch 1,300  of  2,126.    Elapsed: 0:04:04.\n","  Batch 1,350  of  2,126.    Elapsed: 0:04:14.\n","  Batch 1,400  of  2,126.    Elapsed: 0:04:23.\n","  Batch 1,450  of  2,126.    Elapsed: 0:04:33.\n","  Batch 1,500  of  2,126.    Elapsed: 0:04:42.\n","  Batch 1,550  of  2,126.    Elapsed: 0:04:51.\n","  Batch 1,600  of  2,126.    Elapsed: 0:05:01.\n","  Batch 1,650  of  2,126.    Elapsed: 0:05:10.\n","  Batch 1,700  of  2,126.    Elapsed: 0:05:19.\n","  Batch 1,750  of  2,126.    Elapsed: 0:05:29.\n","  Batch 1,800  of  2,126.    Elapsed: 0:05:38.\n","  Batch 1,850  of  2,126.    Elapsed: 0:05:48.\n","  Batch 1,900  of  2,126.    Elapsed: 0:05:57.\n","  Batch 1,950  of  2,126.    Elapsed: 0:06:07.\n","  Batch 2,000  of  2,126.    Elapsed: 0:06:16.\n","  Batch 2,050  of  2,126.    Elapsed: 0:06:25.\n","  Batch 2,100  of  2,126.    Elapsed: 0:06:35.\n","\n","  Average training loss: 0.25\n","  Training epcoh took: 0:06:40\n","\n","Running Validation...\n","  Accuracy: 0.78\n","  Validation Loss: 0.91\n","  Validation took: 0:00:13\n","\n","Training complete!\n","Total training took 0:27:25 (h:mm:ss)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"G-E-GGoWqZnO","outputId":"d25b8aab-a536-44fa-e4e3-ad8e60d478e6"},"source":["import pandas as pd\n","\n","# Display floats with two decimal places.\n","pd.set_option('precision', 2)\n","\n","# Create a DataFrame from our training statistics.\n","df_stats = pd.DataFrame(data=training_stats)\n","\n","# Use the 'epoch' as the row index.\n","df_stats = df_stats.set_index('epoch')\n","\n","# A hack to force the column headers to wrap.\n","#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n","\n","# Display the table.\n","df_stats"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Training Loss</th>\n","      <th>Valid. Loss</th>\n","      <th>Valid. Accur.</th>\n","      <th>Training Time</th>\n","      <th>Validation Time</th>\n","      <th>Valid. Prec</th>\n","      <th>Valid. Recall</th>\n","      <th>Valid. F1</th>\n","    </tr>\n","    <tr>\n","      <th>epoch</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>0.42</td>\n","      <td>0.58</td>\n","      <td>0.79</td>\n","      <td>0:06:37</td>\n","      <td>0:00:13</td>\n","      <td>0.65</td>\n","      <td>0.66</td>\n","      <td>0.61</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.36</td>\n","      <td>0.72</td>\n","      <td>0.78</td>\n","      <td>0:06:38</td>\n","      <td>0:00:13</td>\n","      <td>0.56</td>\n","      <td>0.62</td>\n","      <td>0.55</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.31</td>\n","      <td>0.83</td>\n","      <td>0.80</td>\n","      <td>0:06:40</td>\n","      <td>0:00:13</td>\n","      <td>0.55</td>\n","      <td>0.67</td>\n","      <td>0.56</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.25</td>\n","      <td>0.91</td>\n","      <td>0.78</td>\n","      <td>0:06:40</td>\n","      <td>0:00:13</td>\n","      <td>0.61</td>\n","      <td>0.65</td>\n","      <td>0.59</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       Training Loss  Valid. Loss  Valid. Accur. Training Time  \\\n","epoch                                                            \n","1               0.42         0.58           0.79       0:06:37   \n","2               0.36         0.72           0.78       0:06:38   \n","3               0.31         0.83           0.80       0:06:40   \n","4               0.25         0.91           0.78       0:06:40   \n","\n","      Validation Time  Valid. Prec  Valid. Recall  Valid. F1  \n","epoch                                                         \n","1             0:00:13         0.65           0.66       0.61  \n","2             0:00:13         0.56           0.62       0.55  \n","3             0:00:13         0.55           0.67       0.56  \n","4             0:00:13         0.61           0.65       0.59  "]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"code","metadata":{"id":"EBPlTJ98qZnO","outputId":"43c6fe9e-0096-4af2-e39b-ee969bfec8f7"},"source":["print(np.mean(df_stats['Valid. Accur.']))\n","print(np.mean(df_stats['Valid. Prec']))\n","print(np.mean(df_stats['Valid. Recall']))\n","print(np.mean(df_stats['Valid. F1']))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.7861286919831223\n","0.5947784810126583\n","0.6490330520393812\n","0.5802285208930782\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"scrolled":true,"id":"c0dHffwbqZnO","outputId":"56942a76-9f46-49ad-c7cd-06e2542069f1"},"source":["import matplotlib.pyplot as plt\n","# % matplotlib inline\n","\n","import seaborn as sns\n","\n","# Use plot styling from seaborn.\n","sns.set(style='darkgrid')\n","\n","# Increase the plot size and font size.\n","sns.set(font_scale=1.5)\n","plt.rcParams[\"figure.figsize\"] = (12,6)\n","\n","# Plot the learning curve.\n","plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n","plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n","\n","# Label the plot.\n","plt.title(\"Training & Validation Loss\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.xticks([1, 2, 3, 4])\n","\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 864x432 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"A3HsrsXyqZnP","outputId":"20c61614-6967-47ff-8d99-5b87253bdceb"},"source":["import os\n","base_dir = os.getcwd() + '/'\n","print('Base Dir:', base_dir)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Base Dir: /home/local/ASUAD/mkarami/Documents/Motivational Factors/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XOs7hKf8qZnP","outputId":"32d71724-8285-4675-c210-84fe158dadc1"},"source":["if not os.path.exists(base_dir+'cache/sent_emb_politifact.pt'):\n","    sentence_embedding = []\n","    model.eval()\n","\n","    for i in tqdm(range(len(input_ids_politifact))):\n","        # print(type(input_ids_politifact[0]))\n","        # print(input_ids_politifact.size())\n","        # print(segments_ids[0])\n","\n","        input_ids_politifact = input_ids_politifact.to(device)\n","        # segments_ids = segments_ids.to(device)\n","        # model = model.to(device)\n","        a=torch.reshape(input_ids_politifact[i], (1,len(input_ids_politifact[i])))\n","        # b=torch.reshape(segments_ids[0], (1,len(segments_ids[0])))\n","        # print(b.size())\n","        with torch.no_grad():\n","\n","            outputs = model(a, token_type_ids=None)\n","            # Evaluating the model will return a different number of objects based on \n","            # how it's  configured in the `from_pretrained` call earlier. In this case, \n","            # becase we set `output_hidden_states = True`, the third item will be the \n","            # hidden states from all layers. See the documentation for more details:\n","            # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n","            (logits, hidden_states) = outputs\n","        token_vecs = hidden_states[-2][0]\n","\n","        # Calculate the average of all 22 token vectors.\n","        sentence_embedding.append(torch.mean(token_vecs, dim=0))\n","    torch.save(sentence_embedding, base_dir+'cache/sent_emb_politifact.pt')\n","    \n","else:\n","    print(\"Sent Emb Exists...\")\n","    sentence_embedding = torch.load(base_dir+'cache/sent_emb_politifact.pt')\n","\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|| 18893/18893 [02:31<00:00, 124.43it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"gLXrT4r1qZnP"},"source":["# torch.save(sentence_embedding, '/home/local/ASUAD/mkarami/Documents/cache/'+'sent_emb_politifact')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lKlohbYwqZnP","outputId":"79f349e6-9b8e-47aa-9afc-130991305d48"},"source":["len(sentence_embedding[0])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["768"]},"metadata":{"tags":[]},"execution_count":38}]},{"cell_type":"code","metadata":{"id":"-YEGkdrEqZnP"},"source":["import pickle\n","features_politifact_fake_users = pickle.load(open(base_dir +'/Filtered_date_users_with_three_tweets_or_more/features_politifact_fake_users.pkl', 'rb'))\n","features_politifact_real_users = pickle.load(open(base_dir +'/Filtered_date_users_with_three_tweets_or_more/features_politifact_real_users.pkl', 'rb'))\n","features_politifact_fake_users = features_politifact_fake_users.values()\n","features_politifact_fake_users = [list(x.values()) for x in features_politifact_fake_users]\n","features_politifact_real_users = features_politifact_real_users.values()\n","# features_politifact_real_users = [x.values() for x in features_politifact_real_users]\n","features_politifact_real_users = [list(x.values()) for x in features_politifact_real_users]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HQOdUHkPqZnQ","outputId":"791749dc-0dea-4878-d7e4-f2a57eb435a6"},"source":["len(features_politifact_fake_users)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["6322"]},"metadata":{"tags":[]},"execution_count":40}]},{"cell_type":"code","metadata":{"id":"kyt7Q_dlqZnQ","outputId":"0d2b8219-fa10-46cb-b72d-aca98124f2df"},"source":["politifact_features = []\n","y_labels = []\n","for i in range(len(features_politifact_fake_users)):\n","    politifact_features.append(features_politifact_fake_users[i])\n","    y_labels.append(1)\n","for i in range(len(features_politifact_real_users)):\n","    politifact_features.append(features_politifact_real_users[i])\n","    y_labels.append(0)\n","politifact_features = [[v if v is not None else 0 for v in nested] for nested in politifact_features]\n","# for i in range(len(politifact_features)):\n","politifact_features[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0.006686478454680535,\n"," 0.009658246656760773,\n"," 0.0014858841010401188,\n"," 0.009658246656760773,\n"," 0.0014858841010401188,\n"," 0.0029717682020802376,\n"," 0,\n"," 211,\n"," 5.882352941176471,\n"," 0.30214285714285716,\n"," 0.49]"]},"metadata":{"tags":[]},"execution_count":41}]},{"cell_type":"code","metadata":{"id":"jgUpIOQhqZnQ"},"source":["X_Politifact_concated_ver = []\n","for i in range(len(politifact_features)):\n","    X_Politifact_concated_ver.append(politifact_features[i]+sentence_embedding[i].tolist())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LeC4hd9TqZnQ","outputId":"ce5e84ec-641e-40bf-8f3b-b5e208649651"},"source":["len(X_Politifact_concated_ver[0])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["779"]},"metadata":{"tags":[]},"execution_count":43}]},{"cell_type":"code","metadata":{"id":"e99o9NDGqZnQ"},"source":["# for batch in valid_loader:\n","#     b_input_ids = batch[0].to(device)\n","#     b_input_mask = batch[1].to(device)\n","#     b_labels = batch[2].to(device)\n","#     print(len(b_input_ids))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BKc5hmrmqZnQ","outputId":"ab99e4b3-1407-44f3-977e-10faf9de3ef2"},"source":["from sklearn.neural_network import MLPClassifier\n","from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = train_test_split(X_Politifact_concated_ver, y_labels, test_size=0.1, random_state=42)\n","clf = MLPClassifier(hidden_layer_sizes=(1000,100))\n","clf.fit(X_train, y_train)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["MLPClassifier(hidden_layer_sizes=(1000, 100))"]},"metadata":{"tags":[]},"execution_count":45}]},{"cell_type":"code","metadata":{"id":"XNTRIt7hqZnR","outputId":"36663e19-0251-47af-e425-9ab5191ee1f4"},"source":["from sklearn.metrics import accuracy_score\n","from sklearn.metrics import recall_score\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import f1_score\n","\n","y_pred = clf.predict(X_test)\n","print(\"Acc:\", accuracy_score(y_test, y_pred))\n","print(\"Prec:\", precision_score(y_test, y_pred))\n","print(\"Recall:\", recall_score(y_test, y_pred))\n","print(\"F1:\", f1_score(y_test, y_pred))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Acc: 0.9\n","Prec: 0.9064327485380117\n","Recall: 0.7673267326732673\n","F1: 0.8310991957104559\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TUHPPfRAqZnR"},"source":["## BERT\n","# Acc: 0.7861286919831223\n","# Prec: 0.5947784810126583\n","# Recall: 0.6490330520393812\n","# F1: 0.5802285208930782\n","\n","\n","## BERT + Features\n","# Acc: 0.9\n","# Prec: 0.9064327485380117\n","# Recall: 0.7673267326732673\n","# F1: 0.8310991957104559"],"execution_count":null,"outputs":[]}]}