{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"colab":{"name":"BertTraining - Gossipcop - Submitted.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"N16EQhDrqEjz"},"source":["# Gossipcop"]},{"cell_type":"code","metadata":{"id":"avXAKhDzqEj1"},"source":["import torch\n","\n","input_ids_gossipcop = torch.load('/home/local/ASUAD/mkarami/Documents/Motivational Factors/cache/input_ids_gossipcop_users')\n","attention_masks_gossipcop = torch.load('/home/local/ASUAD/mkarami/Documents/Motivational Factors/cache/attention_masks_gossipcop_users')\n","labels_gossipcop = torch.load('/home/local/ASUAD/mkarami/Documents/Motivational Factors/cache/labels_gossipcop_users')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nWnoST9-qEj1","outputId":"e64e6fdb-39a3-461a-e163-29a6de39bc06"},"source":["from torch.utils.data import TensorDataset, random_split\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","\n","# Combine the training inputs into a TensorDataset.\n","dataset = TensorDataset(input_ids_gossipcop, attention_masks_gossipcop, labels_gossipcop)\n","\n","# Create a 90-10 train-validation split.\n","\n","# Calculate the number of samples to include in each set.\n","train_size = int(0.9 * len(dataset))\n","val_size = len(dataset) - train_size\n","\n","# Divide the dataset by randomly selecting samples.\n","train_dataset, valid_dataset = random_split(dataset, [train_size, val_size])\n","\n","print('{:>5,} training samples'.format(train_size))\n","print('{:>5,} validation samples'.format(val_size))\n","\n","\n","# The DataLoader needs to know our batch size for training, so we specify it \n","# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n","# size of 16 or 32.\n","batch_size = 16\n","\n","# Create the DataLoaders for our training and validation sets.\n","# We'll take training samples in random order. \n","train_loader = DataLoader(\n","            train_dataset,  # The training samples.\n","            sampler = RandomSampler(train_dataset), # Select batches randomly\n","            batch_size = batch_size # Trains with this batch size.\n","        )\n","\n","\n","# For validation the order doesn't matter, so we'll just read them sequentially.\n","valid_loader = DataLoader(\n","            valid_dataset, # The validation samples.\n","            sampler = SequentialSampler(valid_dataset), # Pull out batches sequentially.\n","            batch_size = batch_size # Evaluate with this batch size.\n","        )\n","\n","\n","\n","print('Train = %d (%d) | Validation = %d (%d)' %(len(train_dataset), len(train_loader), len(valid_dataset), len(valid_loader)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["28,993 training samples\n","3,222 validation samples\n","Train = 28993 (1813) | Validation = 3222 (202)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"no4admc1qEj2","outputId":"b2d35546-c1e8-4edf-b2b0-15362edfe4e4"},"source":["# !pip freeze | grep transformers\n","!pip install transformers==\"2.2.0\""],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers==2.2.0 in /home/local/ASUAD/mkarami/anaconda3/lib/python3.8/site-packages (2.2.0)\n","Requirement already satisfied: tqdm in /home/local/ASUAD/mkarami/.local/lib/python3.8/site-packages (from transformers==2.2.0) (4.49.0)\n","Requirement already satisfied: sentencepiece in /home/local/ASUAD/mkarami/.local/lib/python3.8/site-packages (from transformers==2.2.0) (0.1.91)\n","Requirement already satisfied: boto3 in /home/local/ASUAD/mkarami/anaconda3/lib/python3.8/site-packages (from transformers==2.2.0) (1.17.67)\n","Requirement already satisfied: numpy in /home/local/ASUAD/mkarami/.local/lib/python3.8/site-packages (from transformers==2.2.0) (1.19.2)\n","Requirement already satisfied: sacremoses in /home/local/ASUAD/mkarami/.local/lib/python3.8/site-packages (from transformers==2.2.0) (0.0.43)\n","Requirement already satisfied: requests in /home/local/ASUAD/mkarami/anaconda3/lib/python3.8/site-packages (from transformers==2.2.0) (2.24.0)\n","Requirement already satisfied: regex in /home/local/ASUAD/mkarami/.local/lib/python3.8/site-packages (from transformers==2.2.0) (2020.9.27)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/local/ASUAD/mkarami/anaconda3/lib/python3.8/site-packages (from boto3->transformers==2.2.0) (0.10.0)\n","Requirement already satisfied: s3transfer<0.5.0,>=0.4.0 in /home/local/ASUAD/mkarami/anaconda3/lib/python3.8/site-packages (from boto3->transformers==2.2.0) (0.4.2)\n","Requirement already satisfied: botocore<1.21.0,>=1.20.67 in /home/local/ASUAD/mkarami/anaconda3/lib/python3.8/site-packages (from boto3->transformers==2.2.0) (1.20.67)\n","Requirement already satisfied: joblib in /home/local/ASUAD/mkarami/.local/lib/python3.8/site-packages (from sacremoses->transformers==2.2.0) (0.17.0)\n","Requirement already satisfied: click in /home/local/ASUAD/mkarami/anaconda3/lib/python3.8/site-packages (from sacremoses->transformers==2.2.0) (7.1.2)\n","Requirement already satisfied: six in /home/local/ASUAD/mkarami/anaconda3/lib/python3.8/site-packages (from sacremoses->transformers==2.2.0) (1.15.0)\n","Requirement already satisfied: idna<3,>=2.5 in /home/local/ASUAD/mkarami/anaconda3/lib/python3.8/site-packages (from requests->transformers==2.2.0) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /home/local/ASUAD/mkarami/anaconda3/lib/python3.8/site-packages (from requests->transformers==2.2.0) (2020.6.20)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /home/local/ASUAD/mkarami/anaconda3/lib/python3.8/site-packages (from requests->transformers==2.2.0) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/local/ASUAD/mkarami/anaconda3/lib/python3.8/site-packages (from requests->transformers==2.2.0) (1.25.9)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/local/ASUAD/mkarami/anaconda3/lib/python3.8/site-packages (from botocore<1.21.0,>=1.20.67->boto3->transformers==2.2.0) (2.8.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6dtrXre6qEj3","outputId":"1e29a8a0-d743-4843-8823-3184542f3dab"},"source":["from transformers import BertForSequenceClassification, AdamW, BertConfig\n","\n","# Load BertForSequenceClassification, the pretrained BERT model with a single \n","# linear classification layer on top. \n","model = BertForSequenceClassification.from_pretrained(\n","    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n","    num_labels = 2, # The number of output labels--2 for binary classification.\n","                    # You can increase this for multi-class tasks.   \n","    output_attentions = False, # Whether the model returns attentions weights.\n","    output_hidden_states = True, # Whether the model returns all hidden-states.\n",")\n","\n","# Tell pytorch to run this model on the GPU.\n","model.cuda()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"CIKqjre-qEj3"},"source":["# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n","# I believe the 'W' stands for 'Weight Decay fix\"\n","optimizer = AdamW(model.parameters(),\n","                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n","                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n","                )\n","\n","\n","from transformers import get_linear_schedule_with_warmup\n","\n","# Number of training epochs. The BERT authors recommend between 2 and 4. \n","# We chose to run for 4, but we'll see later that this may be over-fitting the\n","# training data.\n","epochs = 4\n","\n","# Total number of training steps is [number of batches] x [number of epochs]. \n","# (Note that this is not the same as the number of training samples).\n","total_steps = len(train_loader) * epochs\n","\n","# Create the learning rate scheduler.\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 0, # Default value in run_glue.py\n","                                            num_training_steps = total_steps)\n","\n","\n","import numpy as np\n","\n","# Function to calculate the accuracy of our predictions vs labels\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n","\n","\n","import time\n","import datetime\n","\n","def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oTdJoXeGqEj3","outputId":"301f27f2-40fb-4665-ade1-ab42c29c9b0d"},"source":["import torch\n","\n","# If there's a GPU available...\n","if torch.cuda.is_available():    \n","\n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","    model = model.to(device)\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["There are 2 GPU(s) available.\n","We will use the GPU: GeForce RTX 2080 Ti\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZRaXi2DgqEj4"},"source":["from sklearn.metrics import recall_score\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import f1_score\n","\n","def flat_precision(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return recall_score(labels_flat, pred_flat)\n","\n","def flat_recall(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return precision_score(labels_flat, pred_flat)\n","\n","def flat_f1_score(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return f1_score(labels_flat, pred_flat)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"luDimhdpqEj4","outputId":"647aff52-283d-49cc-c643-d70a921fb751"},"source":["import random\n","import numpy as np\n","from tqdm import tqdm\n","\n","# This training code is based on the `run_glue.py` script here:\n","# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n","\n","# Set the seed value all over the place to make this reproducible.\n","seed_val = 42\n","\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","# We'll store a number of quantities such as training and validation loss, \n","# validation accuracy, and timings.\n","training_stats = []\n","\n","# Measure the total training time for the whole run.\n","total_t0 = time.time()\n","\n","sentence_embedding_train = []\n","sentence_embedding_val = []\n","\n","\n","# For each epoch...\n","for epoch_i in range(0, epochs):\n","    \n","    # ========================================\n","    #               Training\n","    # ========================================\n","    \n","    # Perform one full pass over the training set.\n","\n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","\n","    # Measure how long the training epoch takes.\n","    t0 = time.time()\n","\n","    # Reset the total loss for this epoch.\n","    total_train_loss = 0\n","\n","    # Put the model into training mode. Don't be mislead--the call to \n","    # `train` just changes the *mode*, it doesn't *perform* the training.\n","    # `dropout` and `batchnorm` layers behave differently during training\n","    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n","    model.train()\n","\n","    # For each batch of training data...\n","    for step, batch in enumerate(train_loader):\n","\n","        # Progress update every 40 batches.\n","        if step % 50 == 0 and not step == 0:\n","            # Calculate elapsed time in minutes.\n","            elapsed = format_time(time.time() - t0)\n","            \n","            # Report progress.\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_loader), elapsed))\n","\n","        # Unpack this training batch from our dataloader. \n","        #\n","        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n","        # `to` method.\n","        #\n","        # `batch` contains three pytorch tensors:\n","        #   [0]: input ids \n","        #   [1]: attention masks\n","        #   [2]: labels \n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","\n","        # Always clear any previously calculated gradients before performing a\n","        # backward pass. PyTorch doesn't do this automatically because \n","        # accumulating the gradients is \"convenient while training RNNs\". \n","        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n","        model.zero_grad()        \n","\n","        # Perform a forward pass (evaluate the model on this training batch).\n","        # The documentation for this `model` function is here: \n","        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","        # It returns different numbers of parameters depending on what arguments\n","        # are given and what flags are set. For our useage here, it returns\n","        # the loss (because we provided labels) and the \"logits\"--the model\n","        # outputs prior to activation.\n","        loss, logits, hidden_states_train = model(b_input_ids, \n","                             token_type_ids=None, \n","                             attention_mask=b_input_mask, \n","                             labels=b_labels)\n","        \n","#         token_vecs = hidden_states_train[-2][0]\n","\n","        # Calculate the average of all 22 token vectors.\n","#         sentence_embedding_train.append(torch.mean(token_vecs, dim=0))\n","        \n","        \n","\n","        # Accumulate the training loss over all of the batches so that we can\n","        # calculate the average loss at the end. `loss` is a Tensor containing a\n","        # single value; the `.item()` function just returns the Python value \n","        # from the tensor.\n","#         print(loss)\n","#         print(type(loss))\n","        total_train_loss += loss.item()\n","        \n","\n","        # Perform a backward pass to calculate the gradients.\n","        loss.backward()\n","\n","        # Clip the norm of the gradients to 1.0.\n","        # This is to help prevent the \"exploding gradients\" problem.\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        # Update parameters and take a step using the computed gradient.\n","        # The optimizer dictates the \"update rule\"--how the parameters are\n","        # modified based on their gradients, the learning rate, etc.\n","        optimizer.step()\n","\n","        # Update the learning rate.\n","        scheduler.step()\n","    \n","    \n","\n","    # Calculate the average loss over all of the batches.\n","    avg_train_loss = total_train_loss / len(train_loader)            \n","    \n","    # Measure how long this epoch took.\n","    training_time = format_time(time.time() - t0)\n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epcoh took: {:}\".format(training_time))\n","        \n","    # ========================================\n","    #               Validation\n","    # ========================================\n","    # After the completion of each training epoch, measure our performance on\n","    # our validation set.\n","\n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    t0 = time.time()\n","\n","    # Put the model in evaluation mode--the dropout layers behave differently\n","    # during evaluation.\n","    model.eval()\n","\n","    # Tracking variables \n","    total_eval_accuracy = 0\n","    total_eval_precision = 0\n","    total_eval_recall = 0\n","    total_eval_f1 = 0\n","    total_eval_loss = 0\n","    nb_eval_steps = 0\n","\n","    # Evaluate data for one epoch\n","    for batch in valid_loader:\n","        \n","        # Unpack this training batch from our dataloader. \n","        #\n","        # As we unpack the batch, we'll also copy each tensor to the GPU using \n","        # the `to` method.\n","        #\n","        # `batch` contains three pytorch tensors:\n","        #   [0]: input ids \n","        #   [1]: attention masks\n","        #   [2]: labels \n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","        \n","        # Tell pytorch not to bother with constructing the compute graph during\n","        # the forward pass, since this is only needed for backprop (training).\n","        with torch.no_grad():        \n","\n","            # Forward pass, calculate logit predictions.\n","            # token_type_ids is the same as the \"segment ids\", which \n","            # differentiates sentence 1 and 2 in 2-sentence tasks.\n","            # The documentation for this `model` function is here: \n","            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","            # Get the \"logits\" output by the model. The \"logits\" are the output\n","            # values prior to applying an activation function like the softmax.\n","            (loss, logits, hidden_states_val) = model(b_input_ids, \n","                                   token_type_ids=None, \n","                                   attention_mask=b_input_mask,\n","                                   labels=b_labels)\n","        \n","#         token_vecs = hidden_states_val[-2][0]\n","\n","            # Calculate the average of all 22 token vectors.\n","#         sentence_embedding_val.append(torch.mean(token_vecs, dim=0))\n","            \n","        # Accumulate the validation loss.\n","#         print(loss)\n","#         print(type(loss))\n","        total_eval_loss += loss.item()\n","\n","        # Move logits and labels to CPU\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","\n","        # Calculate the accuracy for this batch of test sentences, and\n","        # accumulate it over all batches.\n","        total_eval_accuracy += flat_accuracy(logits, label_ids)\n","        total_eval_precision += flat_precision(logits, label_ids)\n","        total_eval_recall += flat_recall(logits, label_ids)\n","        total_eval_f1 += flat_f1_score(logits, label_ids)\n","       \n","\n","    # Report the final accuracy for this validation run.\n","    avg_val_accuracy = total_eval_accuracy / len(valid_loader)\n","    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n","    \n","    avg_val_precision = total_eval_precision / len(valid_loader)\n","    avg_val_recall = total_eval_recall / len(valid_loader)\n","    avg_val_f1 = total_eval_f1 / len(valid_loader)\n","\n","    # Calculate the average loss over all of the batches.\n","    avg_val_loss = total_eval_loss / len(valid_loader)\n","    \n","    # Measure how long the validation run took.\n","    validation_time = format_time(time.time() - t0)\n","    \n","    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n","    print(\"  Validation took: {:}\".format(validation_time))\n","\n","    # Record all statistics from this epoch.\n","    training_stats.append(\n","        {\n","            'epoch': epoch_i + 1,\n","            'Training Loss': avg_train_loss,\n","            'Valid. Loss': avg_val_loss,\n","            'Valid. Accur.': avg_val_accuracy,\n","            'Training Time': training_time,\n","            'Validation Time': validation_time,\n","            'Valid. Prec': avg_val_precision,\n","            'Valid. Recall': avg_val_recall,\n","            'Valid. F1': avg_val_f1,\n","        }\n","    )\n","\n","print(\"\")\n","print(\"Training complete!\")\n","\n","print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","======== Epoch 1 / 4 ========\n","Training...\n","  Batch    50  of  1,813.    Elapsed: 0:00:16.\n","  Batch   100  of  1,813.    Elapsed: 0:00:31.\n","  Batch   150  of  1,813.    Elapsed: 0:00:47.\n","  Batch   200  of  1,813.    Elapsed: 0:01:02.\n","  Batch   250  of  1,813.    Elapsed: 0:01:18.\n","  Batch   300  of  1,813.    Elapsed: 0:01:34.\n","  Batch   350  of  1,813.    Elapsed: 0:01:50.\n","  Batch   400  of  1,813.    Elapsed: 0:02:06.\n","  Batch   450  of  1,813.    Elapsed: 0:02:22.\n","  Batch   500  of  1,813.    Elapsed: 0:02:38.\n","  Batch   550  of  1,813.    Elapsed: 0:02:54.\n","  Batch   600  of  1,813.    Elapsed: 0:03:10.\n","  Batch   650  of  1,813.    Elapsed: 0:03:25.\n","  Batch   700  of  1,813.    Elapsed: 0:03:41.\n","  Batch   750  of  1,813.    Elapsed: 0:03:57.\n","  Batch   800  of  1,813.    Elapsed: 0:04:13.\n","  Batch   850  of  1,813.    Elapsed: 0:04:29.\n","  Batch   900  of  1,813.    Elapsed: 0:04:45.\n","  Batch   950  of  1,813.    Elapsed: 0:05:01.\n","  Batch 1,000  of  1,813.    Elapsed: 0:05:17.\n","  Batch 1,050  of  1,813.    Elapsed: 0:05:32.\n","  Batch 1,100  of  1,813.    Elapsed: 0:05:48.\n","  Batch 1,150  of  1,813.    Elapsed: 0:06:04.\n","  Batch 1,200  of  1,813.    Elapsed: 0:06:20.\n","  Batch 1,250  of  1,813.    Elapsed: 0:06:36.\n","  Batch 1,300  of  1,813.    Elapsed: 0:06:52.\n","  Batch 1,350  of  1,813.    Elapsed: 0:07:08.\n","  Batch 1,400  of  1,813.    Elapsed: 0:07:24.\n","  Batch 1,450  of  1,813.    Elapsed: 0:07:40.\n","  Batch 1,500  of  1,813.    Elapsed: 0:07:56.\n","  Batch 1,550  of  1,813.    Elapsed: 0:08:11.\n","  Batch 1,600  of  1,813.    Elapsed: 0:08:27.\n","  Batch 1,650  of  1,813.    Elapsed: 0:08:43.\n","  Batch 1,700  of  1,813.    Elapsed: 0:08:59.\n","  Batch 1,750  of  1,813.    Elapsed: 0:09:15.\n","  Batch 1,800  of  1,813.    Elapsed: 0:09:31.\n","\n","  Average training loss: 0.21\n","  Training epcoh took: 0:09:35\n","\n","Running Validation...\n","  Accuracy: 0.92\n","  Validation Loss: 0.20\n","  Validation took: 0:00:20\n","\n","======== Epoch 2 / 4 ========\n","Training...\n","  Batch    50  of  1,813.    Elapsed: 0:00:16.\n","  Batch   100  of  1,813.    Elapsed: 0:00:32.\n","  Batch   150  of  1,813.    Elapsed: 0:00:48.\n","  Batch   200  of  1,813.    Elapsed: 0:01:04.\n","  Batch   250  of  1,813.    Elapsed: 0:01:20.\n","  Batch   300  of  1,813.    Elapsed: 0:01:36.\n","  Batch   350  of  1,813.    Elapsed: 0:01:52.\n","  Batch   400  of  1,813.    Elapsed: 0:02:08.\n","  Batch   450  of  1,813.    Elapsed: 0:02:24.\n","  Batch   500  of  1,813.    Elapsed: 0:02:40.\n","  Batch   550  of  1,813.    Elapsed: 0:02:56.\n","  Batch   600  of  1,813.    Elapsed: 0:03:11.\n","  Batch   650  of  1,813.    Elapsed: 0:03:27.\n","  Batch   700  of  1,813.    Elapsed: 0:03:43.\n","  Batch   750  of  1,813.    Elapsed: 0:03:59.\n","  Batch   800  of  1,813.    Elapsed: 0:04:15.\n","  Batch   850  of  1,813.    Elapsed: 0:04:31.\n","  Batch   900  of  1,813.    Elapsed: 0:04:47.\n","  Batch   950  of  1,813.    Elapsed: 0:05:03.\n","  Batch 1,000  of  1,813.    Elapsed: 0:05:19.\n","  Batch 1,050  of  1,813.    Elapsed: 0:05:35.\n","  Batch 1,100  of  1,813.    Elapsed: 0:05:51.\n","  Batch 1,150  of  1,813.    Elapsed: 0:06:07.\n","  Batch 1,200  of  1,813.    Elapsed: 0:06:22.\n","  Batch 1,250  of  1,813.    Elapsed: 0:06:38.\n","  Batch 1,300  of  1,813.    Elapsed: 0:06:54.\n","  Batch 1,350  of  1,813.    Elapsed: 0:07:10.\n","  Batch 1,400  of  1,813.    Elapsed: 0:07:26.\n","  Batch 1,450  of  1,813.    Elapsed: 0:07:42.\n","  Batch 1,500  of  1,813.    Elapsed: 0:07:58.\n","  Batch 1,550  of  1,813.    Elapsed: 0:08:14.\n","  Batch 1,600  of  1,813.    Elapsed: 0:08:30.\n","  Batch 1,650  of  1,813.    Elapsed: 0:08:46.\n","  Batch 1,700  of  1,813.    Elapsed: 0:09:02.\n","  Batch 1,750  of  1,813.    Elapsed: 0:09:18.\n","  Batch 1,800  of  1,813.    Elapsed: 0:09:34.\n","\n","  Average training loss: 0.19\n","  Training epcoh took: 0:09:38\n","\n","Running Validation...\n","  Accuracy: 0.91\n","  Validation Loss: 0.19\n","  Validation took: 0:00:20\n","\n","======== Epoch 3 / 4 ========\n","Training...\n","  Batch    50  of  1,813.    Elapsed: 0:00:16.\n","  Batch   100  of  1,813.    Elapsed: 0:00:32.\n","  Batch   150  of  1,813.    Elapsed: 0:00:48.\n","  Batch   200  of  1,813.    Elapsed: 0:01:04.\n","  Batch   250  of  1,813.    Elapsed: 0:01:20.\n","  Batch   300  of  1,813.    Elapsed: 0:01:36.\n","  Batch   350  of  1,813.    Elapsed: 0:01:52.\n","  Batch   400  of  1,813.    Elapsed: 0:02:08.\n","  Batch   450  of  1,813.    Elapsed: 0:02:24.\n","  Batch   500  of  1,813.    Elapsed: 0:02:40.\n","  Batch   550  of  1,813.    Elapsed: 0:02:56.\n","  Batch   600  of  1,813.    Elapsed: 0:03:12.\n","  Batch   650  of  1,813.    Elapsed: 0:03:28.\n","  Batch   700  of  1,813.    Elapsed: 0:03:45.\n","  Batch   750  of  1,813.    Elapsed: 0:04:01.\n","  Batch   800  of  1,813.    Elapsed: 0:04:17.\n","  Batch   850  of  1,813.    Elapsed: 0:04:33.\n","  Batch   900  of  1,813.    Elapsed: 0:04:49.\n","  Batch   950  of  1,813.    Elapsed: 0:05:05.\n","  Batch 1,000  of  1,813.    Elapsed: 0:05:21.\n","  Batch 1,050  of  1,813.    Elapsed: 0:05:37.\n","  Batch 1,100  of  1,813.    Elapsed: 0:05:53.\n","  Batch 1,150  of  1,813.    Elapsed: 0:06:09.\n","  Batch 1,200  of  1,813.    Elapsed: 0:06:25.\n","  Batch 1,250  of  1,813.    Elapsed: 0:06:41.\n","  Batch 1,300  of  1,813.    Elapsed: 0:06:57.\n","  Batch 1,350  of  1,813.    Elapsed: 0:07:13.\n","  Batch 1,400  of  1,813.    Elapsed: 0:07:29.\n","  Batch 1,450  of  1,813.    Elapsed: 0:07:44.\n","  Batch 1,500  of  1,813.    Elapsed: 0:08:00.\n","  Batch 1,550  of  1,813.    Elapsed: 0:08:16.\n","  Batch 1,600  of  1,813.    Elapsed: 0:08:32.\n","  Batch 1,650  of  1,813.    Elapsed: 0:08:48.\n","  Batch 1,700  of  1,813.    Elapsed: 0:09:04.\n","  Batch 1,750  of  1,813.    Elapsed: 0:09:20.\n","  Batch 1,800  of  1,813.    Elapsed: 0:09:36.\n","\n","  Average training loss: 0.17\n","  Training epcoh took: 0:09:40\n","\n","Running Validation...\n","  Accuracy: 0.90\n","  Validation Loss: 0.23\n","  Validation took: 0:00:20\n","\n","======== Epoch 4 / 4 ========\n","Training...\n","  Batch    50  of  1,813.    Elapsed: 0:00:16.\n","  Batch   100  of  1,813.    Elapsed: 0:00:32.\n","  Batch   150  of  1,813.    Elapsed: 0:00:48.\n","  Batch   200  of  1,813.    Elapsed: 0:01:04.\n","  Batch   250  of  1,813.    Elapsed: 0:01:20.\n","  Batch   300  of  1,813.    Elapsed: 0:01:36.\n","  Batch   350  of  1,813.    Elapsed: 0:01:52.\n","  Batch   400  of  1,813.    Elapsed: 0:02:09.\n","  Batch   450  of  1,813.    Elapsed: 0:02:25.\n","  Batch   500  of  1,813.    Elapsed: 0:02:41.\n","  Batch   550  of  1,813.    Elapsed: 0:02:57.\n","  Batch   600  of  1,813.    Elapsed: 0:03:12.\n","  Batch   650  of  1,813.    Elapsed: 0:03:28.\n","  Batch   700  of  1,813.    Elapsed: 0:03:44.\n","  Batch   750  of  1,813.    Elapsed: 0:04:00.\n","  Batch   800  of  1,813.    Elapsed: 0:04:16.\n","  Batch   850  of  1,813.    Elapsed: 0:04:32.\n","  Batch   900  of  1,813.    Elapsed: 0:04:47.\n","  Batch   950  of  1,813.    Elapsed: 0:05:03.\n","  Batch 1,000  of  1,813.    Elapsed: 0:05:19.\n","  Batch 1,050  of  1,813.    Elapsed: 0:05:34.\n","  Batch 1,100  of  1,813.    Elapsed: 0:05:50.\n","  Batch 1,150  of  1,813.    Elapsed: 0:06:06.\n","  Batch 1,200  of  1,813.    Elapsed: 0:06:22.\n","  Batch 1,250  of  1,813.    Elapsed: 0:06:37.\n","  Batch 1,300  of  1,813.    Elapsed: 0:06:53.\n","  Batch 1,350  of  1,813.    Elapsed: 0:07:09.\n","  Batch 1,400  of  1,813.    Elapsed: 0:07:25.\n","  Batch 1,450  of  1,813.    Elapsed: 0:07:41.\n","  Batch 1,500  of  1,813.    Elapsed: 0:07:56.\n","  Batch 1,550  of  1,813.    Elapsed: 0:08:12.\n","  Batch 1,600  of  1,813.    Elapsed: 0:08:29.\n","  Batch 1,650  of  1,813.    Elapsed: 0:08:45.\n","  Batch 1,700  of  1,813.    Elapsed: 0:09:01.\n","  Batch 1,750  of  1,813.    Elapsed: 0:09:17.\n","  Batch 1,800  of  1,813.    Elapsed: 0:09:32.\n","\n","  Average training loss: 0.15\n","  Training epcoh took: 0:09:36\n","\n","Running Validation...\n","  Accuracy: 0.89\n","  Validation Loss: 0.26\n","  Validation took: 0:00:20\n","\n","Training complete!\n","Total training took 0:39:47 (h:mm:ss)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gh4qa7ChqEj7","outputId":"289fea57-1e56-4320-85ef-ebc024195461"},"source":["import pandas as pd\n","\n","# Display floats with two decimal places.\n","pd.set_option('precision', 2)\n","\n","# Create a DataFrame from our training statistics.\n","df_stats = pd.DataFrame(data=training_stats)\n","\n","# Use the 'epoch' as the row index.\n","df_stats = df_stats.set_index('epoch')\n","\n","# A hack to force the column headers to wrap.\n","#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n","\n","# Display the table.\n","df_stats"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Training Loss</th>\n","      <th>Valid. Loss</th>\n","      <th>Valid. Accur.</th>\n","      <th>Training Time</th>\n","      <th>Validation Time</th>\n","      <th>Valid. Prec</th>\n","      <th>Valid. Recall</th>\n","      <th>Valid. F1</th>\n","    </tr>\n","    <tr>\n","      <th>epoch</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>0.21</td>\n","      <td>0.20</td>\n","      <td>0.92</td>\n","      <td>0:09:35</td>\n","      <td>0:00:20</td>\n","      <td>0.98</td>\n","      <td>0.93</td>\n","      <td>0.95</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.19</td>\n","      <td>0.19</td>\n","      <td>0.91</td>\n","      <td>0:09:38</td>\n","      <td>0:00:20</td>\n","      <td>0.97</td>\n","      <td>0.94</td>\n","      <td>0.95</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.17</td>\n","      <td>0.23</td>\n","      <td>0.90</td>\n","      <td>0:09:40</td>\n","      <td>0:00:20</td>\n","      <td>0.95</td>\n","      <td>0.94</td>\n","      <td>0.94</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.15</td>\n","      <td>0.26</td>\n","      <td>0.89</td>\n","      <td>0:09:36</td>\n","      <td>0:00:20</td>\n","      <td>0.93</td>\n","      <td>0.95</td>\n","      <td>0.94</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       Training Loss  Valid. Loss  Valid. Accur. Training Time  \\\n","epoch                                                            \n","1               0.21         0.20           0.92       0:09:35   \n","2               0.19         0.19           0.91       0:09:38   \n","3               0.17         0.23           0.90       0:09:40   \n","4               0.15         0.26           0.89       0:09:36   \n","\n","      Validation Time  Valid. Prec  Valid. Recall  Valid. F1  \n","epoch                                                         \n","1             0:00:20         0.98           0.93       0.95  \n","2             0:00:20         0.97           0.94       0.95  \n","3             0:00:20         0.95           0.94       0.94  \n","4             0:00:20         0.93           0.95       0.94  "]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"KB_WO5VuqEj8","outputId":"e2840a90-c269-47bb-934e-110bd9035501"},"source":["print(np.mean(df_stats['Valid. Accur.']))\n","print(np.mean(df_stats['Valid. Prec']))\n","print(np.mean(df_stats['Valid. Recall']))\n","print(np.mean(df_stats['Valid. F1']))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.903697400990099\n","0.95817528696551\n","0.9379749066939664\n","0.946140450071429\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mrTIvbr1qEj9","outputId":"0a9bd747-d674-40fc-91eb-ff85201c2885"},"source":["import os\n","if not os.path.exists('/home/local/ASUAD/mkarami/Documents/Motivational Factors/cache/'+'sent_emb_gossipcop.pt'):\n","    sentence_embedding = []\n","    model.eval()\n","\n","    for i in tqdm(range(len(input_ids_gossipcop))):\n","\n","\n","        input_ids_gossipcop = input_ids_gossipcop.to(device)\n","        a=torch.reshape(input_ids_gossipcop[i], (1,len(input_ids_gossipcop[i])))\n","\n","        with torch.no_grad():\n","\n","            outputs = model(a, token_type_ids=None)\n","            # Evaluating the model will return a different number of objects based on \n","            # how it's  configured in the `from_pretrained` call earlier. In this case, \n","            # becase we set `output_hidden_states = True`, the third item will be the \n","            # hidden states from all layers. See the documentation for more details:\n","            # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n","            (logits, hidden_states) = outputs\n","        token_vecs = hidden_states[-2][0]\n","\n","        # Calculate the average of all 22 token vectors.\n","        sentence_embedding.append(torch.mean(token_vecs, dim=0))\n","    torch.save(sentence_embedding, '/home/local/ASUAD/mkarami/Documents/Motivational Factors/cache/'+'sent_emb_gossipcop')\n","else:\n","    sentence_embedding = torch.load('/home/local/ASUAD/mkarami/Documents/Motivational Factors/cache/'+'sent_emb_gossipcop.pt')\n","\n","\n"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'model' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-15d9f04f2116>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/local/ASUAD/mkarami/Documents/Motivational Factors/cache/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'sent_emb_gossipcop.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msentence_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids_gossipcop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"]}]},{"cell_type":"code","metadata":{"id":"pphuHg65qEj9"},"source":["import torch\n","sentence_embedding = torch.load('/home/local/ASUAD/mkarami/Documents/Motivational Factors/cache/'+'sent_emb_gossipcop')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c5NOjv0KqEj9","outputId":"18268507-9f87-4c0b-eb18-e2dd2865a8a0"},"source":["import os\n","base_dir = os.getcwd() + '/'\n","print('Base Dir:', base_dir)\n","\n","import pickle\n","features_gossipcop_fake_users = pickle.load(open(base_dir +'/Filtered_date_users_with_three_tweets_or_more/features_gossipcop_fake_users.pkl', 'rb'))\n","features_gossipcop_fake_users = features_gossipcop_fake_users.values()\n","features_gossipcop_real_users = pickle.load(open(base_dir +'/Filtered_date_users_with_three_tweets_or_more/features_gossipcop_real_users.pkl', 'rb'))\n","features_gossipcop_real_users = features_gossipcop_real_users.values()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Base Dir: /home/local/ASUAD/mkarami/Documents/Motivational Factors/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oo9ubizwqEj9"},"source":["gossipcop_features = []\n","for i in range(len(list(features_gossipcop_fake_users))):\n","    gossipcop_features.append(list(list(features_gossipcop_fake_users)[i].values()))\n","for i in range(len(list(features_gossipcop_real_users))):\n","    gossipcop_features.append(list(list(features_gossipcop_real_users)[i].values()))\n","gossipcop_features = [[v if v is not None else 0 for v in nested] for nested in gossipcop_features]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dpGI6UO1qEj-"},"source":["X_Gossipcop_concated_ver = []\n","for i in range(len(gossipcop_features)):\n","    X_Gossipcop_concated_ver.append(gossipcop_features[i]+sentence_embedding[i].tolist())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q_HwdXeYqEj-","outputId":"c303764c-85fe-45f9-b360-863a5c27ceab"},"source":["from sklearn.neural_network import MLPClassifier\n","from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = train_test_split(X_Gossipcop_concated_ver, labels_gossipcop.tolist(), test_size=0.33, random_state=42)\n","clf = MLPClassifier(hidden_layer_sizes=(1000,500))\n","clf.fit(X_train, y_train)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["MLPClassifier(hidden_layer_sizes=(1000, 500))"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"xyarNgZyqEj-","outputId":"5aa0f7c5-cf52-46a5-fde9-2a6536ddb8b9"},"source":["from sklearn.metrics import accuracy_score\n","from sklearn.metrics import recall_score\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import f1_score\n","\n","y_pred = clf.predict(X_test)\n","print(accuracy_score(y_test, y_pred))\n","print(recall_score(y_test, y_pred))\n","print(precision_score(y_test, y_pred))\n","print(f1_score(y_test, y_pred))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.9269118615370144\n","0.9637055315648776\n","0.9567214453404104\n","0.9602007888131947\n"],"name":"stdout"}]}]}